%
% File acl2014.tex
%
% Contact: giovanni.colavizza@epfl.ch
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn,
%% based on the style files for ACL-2010, which were, in turn,
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}


\setlength\titlebox{6cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Evolution of direct democracy in Switzerland}

\author{Youssef Janjar \\
  {\tt youssef.janjar@epfl.ch} \\\\
  \textbf{Pierre Mbanga Ndjock} \\
  {\tt pierre.mbangandjock@epfl.ch} \\\\
\textbf{Robin Denhardt-Eriksson} \\
{\tt robin.denhardteriksson@epfl.ch} \\}

\date{19/12/2017}

\begin{document}
\maketitle
\begin{abstract}
  This document contains the instructions for preparing a report for ADA 2017. The document itself conforms to its own specifications, and is therefore an example of
  what your manuscript should look like. This document is based on the ACL 2014 paper format.
\end{abstract}

\bigskip

% a mon avis il y a beaucoup trop d'introduction sur la suisse et pas assez de data analysis
\section{Introduction}

Due to its federal constitution the Swiss Confederation has forged itself
a solid reputation of placing high importance on individual freedoms and democracy.
% Beaucoup trop over-the-top
% The land is praised by observers and often regarded as a model to follow.
One specificity of this state relies in the voting frequency.
Swiss citizens are regularly called to the polls. This aspect amongst others constitutes one fundamental
argument showing the 'democraticity' of the country.
\\
Votations, as they are named in the country, can be initiated by any citizen. These can take place at
three different levels of the confederation, namely communal, cantonal or federal. Communal votations
are reserved for swiss residents inside a given commune. The same apply for cantonal votations where eligible citizen
must reside in the canton. Finally, federal votations concern all citizen in the country. This system
allows any citizen to submit a votation to his or her fellow citizens, under a specific set of criteria.
\\
We want to challenge this polished reputation acquired over the years by gaining some insights into the
process of direct democracy and various
subjects that have been addressed.

\section{Project Goal}

In the scope of this project we will use written political articles in newspapers to assess
the diversity of subjects proposed to votations to Swiss residents over the last 200 years. We essentially
want to classify political articles in order to identify trends, distributions, densities or patterns
over the decades. This approach should allow us to gain valuable insight into concerns that underlie
the Swiss political landscape.

\section{Dataset}

We seek to analyze "Le Temps digital archives and data". This dataset consists of articles representing two centuries of informations provided by two newspapers, namely 'Le journal de Geneve' and 'La Gazette de Lausanne'. These are ancestors of today's well-known Swiss newspaper 'Le Temps' whose publications are written in the French language.

The dataset at hand is very well organized. It is easy to navigate and find articles for any given time
period as publications are gathered with respect to their release month and year. Hence for each newspaper all articles published during a specific month of a given year are all stored in the same file. The data structure used in each of those files is xml.

Unfortunately some texts are unreadable as they contain a bunch of symbols or special characters. This observation mostly arises in old publications. This may be caused by the scanning and image recognition process used to create the digital dataset, not adapted for those fonts, or degraded original copies. As an example, the following article issued on February 1798:\\
\\
\textit{[...] JLEfauteuil c \$ t vacc'ant, et l'Assemblée s'ôccûpé * de la nomination d'un nouveau Président. Une immense majoritS y rappelle le C. Glayre " J'a * vois" besoin de forces [...]}\\
\\
Specifically the dataset consists of 4'335 xml files for a total of () articles. The period of time covered by those articles ranges from February 1798 to February 1998.


\section{Data preparation}

Naturally we apply tools related to the field of natural language processing to acquire our results. More
precisely we are interested in topic modeling using unsupervised machine learning algorithms. It has been essential to develop
a sensible pipeline to improve the results of the algorithm. Therefore we have modeled this pipeline into 4 different stages that we detail in subsequent section: data retrieval, data reduction, data cleaning and data processing.
\\
Due to its performance and the variety of implementations available we decide to use Latent Dirichlet Allocation (LDA).
\subsection{Data retrieval}

Our first task is to extract and store the articles from the xml files.
For each article in the files we save only the raw text and the date of publication.\\
It is important to mention here that the articles inside the xml structure do not consistently represent the natural classification by theme as we may expect. As an example let's consider the following publication issued on 04 September 1993:\\ % rajouter le journal d'ou vient la publication

\textit{\textbf{QUOTIDIENNES JUSTICE} Cadres du bâtiment acquittés Le Tribunal correctionnel de Lausanne a acquitté jeudi deux anciens cadres [...] \textbf{CONJONCTURE} Vaud toujours malade La marche des affaires de l'industrie vaudoise est anémique. L'entrée des commandes [...]\textbf{VOTATIONS} Deux oui et trois non libéraux Réunis à Lausanne, les délégués du Parti libéral vaudois ont pris position sur les cinq objets soumis à la votation populaire des 26 et 27 septembre prochains. [...] \textbf{PAYSANS DE MONTAGNE} Aide du gouvernement Face à la grogne suscitée chez les paysans de montagne [...]}\\

The entire text is considered a single entity inside the corresponding xml file altough we can clearly discriminate four different topics, namely 'Quotidiennes de Justice', 'Conjoncture', 'Votation' and 'Paysan de montagne'. This remark motivates a choice we explain in the next subsection.

\subsection{Data reduction}

In order to filter articles related to votations we take a very simple approach. We argue that it is very unlikely that any publication related to Swiss votations does not contain at least one word in a set of predefined words. Our chosen set of words we consider as strongly related to votations: 'votation','voter','referendum','election','initiative populaire','grand conseil','plebiscite','scrutin','suffrage'. Those words are obtained based on our personal intuition and initial exploration of the dataset. Of course, One can argue about the accuracy of such a method. We may certainly generate false positives or discard relevant articles. Yet the sample size we obtain with this first process appears to be large enough to capture the information we are interested in. Filtering the entire dataset according to these keywords leads to a reduced corpus of 359,038 articles.
\\
Furthermore, we make an assumption that reduces the length of any given publication related to votations. As mentionned earlier the goal is to identify and classify votation subjects. By visual inspection, we notice this information is usually closed to one of the keywords showed earlier.
Let's consider the following extract of a publication issued on 04 September 1993:\\
\\
\textit{VOTATIONS Deux oui et trois non libéraux Réunis à Lausanne, les délégués du Parti libéral vaudois ont pris position sur les cinq objets soumis à la \textbf{votation} populaire des 26 et 27 septembre prochains. C'est ainsi qu'ils ont accepté le rattachement du district de Laufon à Bâle-Campagne et l'arrêté fédéral urgent en matière \textbf{d'assurance-maladie}. En revanche, l'arrêté fédéral contre l'usage abusif \textbf{d'armes} et l'initative \textbf{« Pour un jour de Fête national férié »} n'ont pas trouvé grâce à leurs yeux. Pas plus d'ailleurs que l'arrêté fédéral urgent en matière \textbf{d'assurance chômage}, jugé trop coûteux pour l'économie.}\\

This extract is part of the article presented in the previous subsection. It represents approximately twenty percent of the original in terms of characters. Yet it is more than enough to capture the subjects of the votations, highlighted in bold. More importantly, the remaining part of the article does not provide any insight for the information we are looking for. We can even argue that it is pure noise that needs to be discarded. As a consequence we retain only sentences containing one of our defined keywords together with its closest neighbours, namely the preceding and following sentences. Again we are aware that this filtering method may discard relevant information. More importantly, this assumption may reveal disastruous for our results if it appears to be erroneous.


\subsection{Data cleaning}

At this stage we have stored articles that have been filtered and stored in an DataFrame with their publication date and raw text. Our objective is now to pre-process those publications before running the unsupervised learning method. The pipeline we use for this task can be summarized by the following schema
\\
\\
\textit{remove non French words $\rightarrow$ lemmatize $\rightarrow$ remove stop words $\rightarrow$ remove digits}
\\

% C'est vraiment recommande par NLTK? Ok
To remove non French words we use a dictionary proposed by the NLTK library: the french dictionary of the PyEnchant spellchecking library. The lemmatization step is resolved through the Spacy lemmatizer. Spacy also proposes functionalities to easily detect and remove stop words and digits. All these actions put together return a cleaner corpus that can be processed more efficiently and hopefully lead to more intelligible results.

\section{Data processing}

The pre-processing stages described in the previous section can now be exploited to derive results. As mentioned earlier the task we are interested in is topic modeling. The problem constraints force us to use an unsupervised learning method for clustering known as Latent Dirichlet Allocation (LDA). This choice is motivated by the ADA lectures, discussions with peers and practical experiment results observed in the literature. % rajouter une reference ou deux a des articles
Other clustering methods such as K-means or DBSCAN are not specifically orientated to problems involving natural language processing, hence why they were discarded.

\subsection{Latent Dirichlet allocation}
% rajouter une reference pour wikipedia
According to Wikipedia, latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar.
From a practical point of view, LDA represents each document as linear combination of topics. Each of these topics have a certain probability of generating different words. Documents are reduced using the bag-of-words representation. LDA then tries to backtrack from the documents to find a set of topics that are likely to have generated the corpus.

\label{sect:pdf}

\subsection{Graphics}

{\bf Illustrations}: Place figures, tables, and photographs in the
report near where they are first discussed, rather than at the end, if
possible.  Wide illustrations may run across both columns.

{\bf Captions}: Provide a caption for every illustration; number each one
sequentially in the form:  ``Figure 1. Caption of the Figure.'' ``Table 1.
Caption of the Table.''  Type the captions of the figures and
tables below the body, using 11 point text.
\section{Libraries}
Pandas was of course used for organising data.
The two other main libraries used are Spacy and Gensim.
We used Spacy for pre-processing our data. Articles were lemmatized using the fonction 'lemma', we used it also to drop the stop words, the digits and the words that were not french.
We used Gensim to implemente of the LDA algorithm for the text classification.
pyLDAvis was used as a visual representation tool for the results of LDA.


\begin{thebibliography}{}

\bibitem[\protect\citename{Hamed Jelodar, Yongli Wang, Chi Yuan, Xia Feng}]{Aho:72}
Department of Computer Science and Engineering Nanjing University of Science and Technology,.
\newblock 1972.
\newblock {\em Latent Dirichlet Allocation (LDA) and Topic modeling: models, applications, a survey}.


\bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
Dan Gusfield.
\newblock 1997.
\newblock {\em Algorithms on Strings, Trees and Sequences}.
\newblock Cambridge University Press, Cambridge, UK.

\end{thebibliography}

\end{document}
