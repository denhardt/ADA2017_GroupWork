{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from datetime import datetime\n",
    "from article_selection import article_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def month_dates(start, end):\n",
    "    f = lambda date: date.month + 12 * date.year\n",
    "\n",
    "    res = []\n",
    "    for tot_m in range(f(start)-1, f(end)):\n",
    "        y, m = divmod(tot_m, 12)\n",
    "        res.append(str(y) + '/' + '%02d' % (m+1))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_date(article):\n",
    "    \"\"\"\n",
    "    This method returns the date of the article\n",
    "    \"\"\"\n",
    "    str_date = article.find('entity').find('meta').find('issue_date').text\n",
    "    return datetime.strptime(str_date, '%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_articles_in_file(file, start_date, end_date):\n",
    "    articles = []  \n",
    "    for article in file.iter('article'):\n",
    "        if article.find('entity') is not None:\n",
    "            a = ''\n",
    "            date = get_date(article)\n",
    "            if start_date <= date <= end_date:\n",
    "                for entity in article.iter('entity'):\n",
    "                    a += entity.findtext('full_text') + ' '\n",
    "                articles.append(date.strftime('%d/%m/%Y') + ' ' + a)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_articles(path, start_date, end_date):\n",
    "    articles = []\n",
    "    for m_date in month_dates(start_date, end_date):\n",
    "        try:\n",
    "            file = etree.parse(path + m_date + '.xml')\n",
    "            articles.append(get_articles_in_file(file, start_date, end_date))\n",
    "        except (FileNotFoundError, IOError):\n",
    "            pass\n",
    "    return [a for file in articles for a in file]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_text(file, box_id):\n",
    "    res = None\n",
    "    for article in file.iter('article'):\n",
    "        if article.find('entity') is not None:\n",
    "            date = get_date(article)\n",
    "            for entity in article.iter('entity'):\n",
    "                if   box_id == entity.find('meta').find('box').text:\n",
    "                    res = date.strftime('%d/%m/%Y') + ' ' + entity.findtext('full_text')\n",
    "                    break\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/mbanga/Desktop/JDG/'\n",
    "start_date =  datetime(1990, 1, 1)\n",
    "end_date = datetime(1998, 2, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles = get_articles(path, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "358455"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fr_core_news_sm\n",
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = fr_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuaiton or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_french(word):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens that\n",
    "    are not french words.\n",
    "    \"\"\"\n",
    "    d = enchant.Dict('fr_FR')\n",
    "    return d.check(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lemmatized_corpus(corpus):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse articles,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    j = 0\n",
    "    i = 0\n",
    "    for parsed_article in nlp.pipe(corpus, \n",
    "                                   batch_size=100, n_threads=5):\n",
    "        # save the date\n",
    "        date = parsed_article[0].text\n",
    "        \n",
    "        yield (date, ' '.join([token.lemma_ for token in parsed_article\n",
    "                             if not punct_space(token) and is_french(token.text)\n",
    "                                and not token.is_stop and not token.is_digit\n",
    "                                and not token.like_num]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_votation(articles, lemmas):\n",
    "    votations = []\n",
    "    for article in articles:\n",
    "        if any(lemma in article for lemma in lemmas): \n",
    "            votations.append(article)\n",
    "    return votations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def corpus_votation_bis(articles, lemmas):\n",
    "    votations = []\n",
    "    for article in articles:\n",
    "        if any(lemma in article.replace(' ', '') for lemma in lemmas):\n",
    "            votations.append(article)\n",
    "    return votations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive selection (First Filtering)\n",
    "lems = ['votation', 'referendum']\n",
    "\n",
    "#articles_votation_third = corpus_votation_bis(articles, lemmas)\n",
    "#articles_votation_bis = corpus_votation(articles, lemmas)\n",
    "\n",
    "articles_votation = article_selection(articles, lems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2561"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles_votation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    %%time\n",
    "    # Time consuming !!\n",
    "    lemmatized_corpus = [(date, lemmas) for date, lemmas in lemmatized_corpus(articles_votation)]\n",
    "\n",
    "    # retrieve dates\n",
    "    dates = [pair[0] for pair in lemmatized_corpus]\n",
    "\n",
    "    # retrieve articles\n",
    "    corpus = [pair[1] for pair in lemmatized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2561"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lemmatized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    project_path = '/home/mbanga/Epfl/AppliedDataAnalysis/ADA2017_GroupWork/Project/'\n",
    "\n",
    "    with open(os.path.join(project_path, 'lemmatized articles 1990-1998.txt'), 'w') as file:\n",
    "        for article in corpus:\n",
    "            file.write(article + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    # check ouput of lemmatizer (lemmatized_corpus) \n",
    "    file = etree.parse('/home/mbanga/Desktop/JDG/1990/01.xml')\n",
    "    box_id = '24 123 1446 2167'\n",
    "\n",
    "    original_text = [get_entity_text(file, box_id)]\n",
    "\n",
    "    for lemmatized in lemmatized_corpus(original_text):\n",
    "        print(lemmatized[1], '\\n')\n",
    "    print(original_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    # check naive selection\n",
    "    file = etree.parse('/home/mbanga/Desktop/JDG/1990/01.xml')\n",
    "    box_id = '50 163 1090 888'\n",
    "\n",
    "    original_text = [get_entity_text(file, box_id)]\n",
    "    lemmas = ['vote', 'voter', 'votation', 'referendum']\n",
    "    res = corpus_votation(original_text, lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "#import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# learn the dictionnary by iterating over all of the articles\n",
    "dico = Dictionary([article.split() for article in corpus])\n",
    "\n",
    "# filter tokens that are very rare or too common from\n",
    "# the dictionary \n",
    "dico.filter_extremes(no_below=0, no_above=0.4)\n",
    "\n",
    "# reassign integer lda\n",
    "dico.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_generator(corpus):\n",
    "    \"\"\"\n",
    "    generator function to read articles from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    for article in corpus:\n",
    "        yield dico.doc2bow(article.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate bag-of-word representations for\n",
    "# all reviews and save them as a matrix\n",
    "project_path = '/home/mbanga/Epfl/AppliedDataAnalysis/ADA2017_GroupWork/Project/'\n",
    "MmCorpus.serialize(os.path.join(project_path, 'corpus.mm'),\n",
    "                                bow_generator(corpus))\n",
    "\n",
    "bow_corpus = MmCorpus(os.path.join(project_path, 'corpus.mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model_filepath = os.path.join(project_path, 'lda_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "        # workers => sets the parallelism, and should be\n",
    "        # set to your number of physical cores minus one\n",
    "        lda = LdaMulticore(bow_corpus,\n",
    "                           num_topics=5,\n",
    "                           id2word=dico,\n",
    "                           workers=5)\n",
    "        \n",
    "        lda.save(lda_model_filepath)\n",
    "\n",
    "#load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def explore_topic(topic_number, topn=25):\n",
    "    \"\"\"\n",
    "    accept a user-supplied topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "    \n",
    "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "    \n",
    "    for term, frequency in lda.show_topic(topic_number, topn):\n",
    "        print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term                 frequency\n",
      "\n",
      "initiative           0.005\n",
      "européen             0.004\n",
      "loi                  0.003\n",
      "genève               0.003\n",
      "une                  0.003\n"
     ]
    }
   ],
   "source": [
    "explore_topic(topic_number=0, topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal is to find all documents related to the same topic\n",
    "def articles_topic(lda, bow_corpus, corpus, topic):\n",
    "    \"\"\"\n",
    "    return the list of articles associated\n",
    "    with a given topic.\n",
    "    \"\"\"\n",
    "    assert len(bow_corpus) == len(corpus)\n",
    "    nb_topics = len(lda.get_topics())\n",
    "    \n",
    "    documents = []\n",
    "    if 0 <= topic < nb_topics:\n",
    "        k = 0\n",
    "        for bow_article in bow_corpus:\n",
    "            dist = lda.get_document_topics(bow_article, minimum_probability=0)\n",
    "            dist = [p[1] for p in dist]\n",
    "            idx_max = dist.index(max(dist))\n",
    "            if idx_max == topic:\n",
    "                documents.append(corpus[k])\n",
    "            k += 1\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = articles_topic(lda, bow_corpus, articles_votation, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "394"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
