{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_dates(start, end):\n",
    "    f = lambda date: date.month + 12 * date.year\n",
    "\n",
    "    res = []\n",
    "    for tot_m in range(f(start)-1, f(end)):\n",
    "        y, m = divmod(tot_m, 12)\n",
    "        res.append(str(y) + '/' + '%02d' % (m+1))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_date(article):\n",
    "    \"\"\"\n",
    "    This method returns the date of the article\n",
    "    \"\"\"\n",
    "    str_date = article.find('entity').find('meta').find('issue_date').text\n",
    "    return datetime.strptime(str_date, '%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_articles_in_file(file, start_date, end_date):\n",
    "    articles = []  \n",
    "    for article in file.iter('article'):\n",
    "        if article.find('entity') is not None:\n",
    "            a = ''\n",
    "            date = get_date(article)\n",
    "            if start_date <= date <= end_date:\n",
    "                for entity in article.iter('entity'):\n",
    "                    a += entity.findtext('full_text') + ' '\n",
    "                articles.append(date.strftime('%d/%m/%Y') + ' ' + a)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_articles(path, start_date, end_date):\n",
    "    articles = []\n",
    "    for m_date in month_dates(start_date, end_date):\n",
    "        try:\n",
    "            file = etree.parse(path + m_date + '.xml')\n",
    "            articles.append(get_articles_in_file(file, start_date, end_date))\n",
    "        except (FileNotFoundError, IOError):\n",
    "            pass\n",
    "    return [a for file in articles for a in file]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"def get_articles(path, start_date, end_date):\\n    articles = []\\n    for m_date in month_dates(start_date, end_date):\\n        try:\\n            parser = etree.parse(path + m_date + '.xml')\\n\\n            for article in parser.iter('article'):\\n                if article.find('entity') is not None:\\n                    a = ''\\n                    date = get_date(article)\\n                    if start_date <= date <= end_date:\\n                        for entity in article.iter('entity'):\\n                            a += entity.findtext('full_text') + ' '\\n                        articles.append(date.strftime('%d/%m/%Y') + ' ' + a)\\n                    \\n        except (FileNotFoundError, IOError):\\n            pass\\n    return articles\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def get_articles(path, start_date, end_date):\n",
    "    articles = []\n",
    "    for m_date in month_dates(start_date, end_date):\n",
    "        try:\n",
    "            parser = etree.parse(path + m_date + '.xml')\n",
    "\n",
    "            for article in parser.iter('article'):\n",
    "                if article.find('entity') is not None:\n",
    "                    a = ''\n",
    "                    date = get_date(article)\n",
    "                    if start_date <= date <= end_date:\n",
    "                        for entity in article.iter('entity'):\n",
    "                            a += entity.findtext('full_text') + ' '\n",
    "                        articles.append(date.strftime('%d/%m/%Y') + ' ' + a)\n",
    "                    \n",
    "        except (FileNotFoundError, IOError):\n",
    "            pass\n",
    "    return articles\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# retrieve the articles from JDL dataset\\nstart = date(1826, 1, 1); end = date(1998, 2, 28)\\narticles = []\\nfor date in month_dates(start, end):\\n    path = '/home/mbanga/Desktop/JDG/' + date + '.xml'\\n    try\\n        parser = etree.parse(path)\\n        tag_text = 'full_text'\\n\\n        for article in parser.getroot().iterchildren():\\n            a = ''\\n            for elem in article.iterchildren():\\n                a += elem.findtext(tag_text) + ' '\\n            articles.append(a)\\n\\n    except (FileNotFoundError, IOError):\\n        pass\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# retrieve the articles from JDL dataset\n",
    "start = date(1826, 1, 1); end = date(1998, 2, 28)\n",
    "articles = []\n",
    "for date in month_dates(start, end):\n",
    "    path = '/home/mbanga/Desktop/JDG/' + date + '.xml'\n",
    "    try\n",
    "        parser = etree.parse(path)\n",
    "        tag_text = 'full_text'\n",
    "\n",
    "        for article in parser.getroot().iterchildren():\n",
    "            a = ''\n",
    "            for elem in article.iterchildren():\n",
    "                a += elem.findtext(tag_text) + ' '\n",
    "            articles.append(a)\n",
    "\n",
    "    except (FileNotFoundError, IOError):\n",
    "        pass\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/home/mbanga/Desktop/JDG/'\n",
    "start_date =  datetime(1826, 1, 1)\n",
    "end_date = datetime(1998, 2, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = get_articles(path, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2997628"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = articles[-50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_article = nlp(test_corpus[48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1:\n",
      "28/02/1998 Mœurs Pamela Anderson demande (encore) le divorce Pamela Anderson a demandé jeudi à divorcer de son mari, le batteur Tommy Lee. Ce dernier a été arrêté deux jours plus tôt à leur domicile pour avoir agressé l'actrice. Il s'agit de ta deuxième requête en divorce déposée par Pamela Anderson en deux ans. Le couple s'était réconcilié après la naissance de leur premier fils, Brandon, et Pamela Anderson avait dit qu'elle voulait aider son époux à vaincre son alcoolisme. Selon l'actrice, Tommy Lee l'a attaquée alors qu'elle ber-çait leur fils de sept semaines, Dylan Jagger. Elle a été légèrement blessée à la main et présente des marques rouges sur le dos. La police appelée à leur domicile de Malibu y a découvert une arme, en violation du sursis avec mise à l'épreuve auquel le batteur était soumis pour avoir agressé un photographe. (ATS / Reuter) Vin Attention aux mélanges... Le Tribunal de Sierre a confirmé vendredi la peine de trois mois de prison avec sursis contre un encaveur qui avait mélangé un cru valaisan à un vin espagnol. Il a rejeté l'appel formulé par l'encaveur et confirmé la peine prononcée en première instance. L'homme avait mélangé du pinot noir valaisan à du vin espagnol et vendu le breuva ge ainsi obtenu sous l'appellation Appellation d'origine contrôlée (AOC) Valais. Il avait été condamné pour falsification de marchandise en première instance, mais a fait recours, estimant n'avoir commis aucun acte illicite. Le tribunal a confirmé la première peine. Il a en outre condamné l'encaveur à verser à l'Etat du Valais une créance de 12 ooo francs, soit l'équivalent du bénéfice estimé de l'opération. (ATS) Viande séchée des Grisons L'Allemagne assouplit sa position Après quatre ans de controverse, l'Allemagne tolère la présence de salpêtre dans la viande séchée des Grisons. Bonn s'est aligné sur la position européenne, qui admet le processus de fabrication suisse. Le salpêtre est considéré comme un additif indispensable dans la pré-paration de la viande des Grisons. Le marché allemand est un débouché important pour le secteur de la viande séchée. (ATS)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for num, sentence in enumerate(parsed_article.sents):\n",
    "    print('Sentence {}:' .format(num + 1))\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>token_pos</th>\n",
       "      <th>token_shape</th>\n",
       "      <th>token_oov</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28/02/1998</td>\n",
       "      <td>NUM</td>\n",
       "      <td>dd/dd/dddd</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mœurs</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pamela</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anderson</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>Xxxxx</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>demande</td>\n",
       "      <td>VERB</td>\n",
       "      <td>xxxx</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token_text token_pos token_shape  token_oov\n",
       "0  28/02/1998       NUM  dd/dd/dddd       True\n",
       "1       Mœurs     PROPN       Xxxxx       True\n",
       "2      Pamela     PROPN       Xxxxx       True\n",
       "3    Anderson     PROPN       Xxxxx       True\n",
       "4     demande      VERB        xxxx       True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_pos = [token.pos_ for token in parsed_article]\n",
    "token_text = [token.orth_ for token in parsed_article]\n",
    "token_shape = [token.shape_ for token in parsed_article]\n",
    "token_oov = [token.is_oov for token in parsed_article]\n",
    "\n",
    "t = pd.Series(token_text, name='token_text')\n",
    "p = pd.Series(token_pos, name='token_pos')\n",
    "s = pd.Series(token_shape, name='token_shape')\n",
    "oov = pd.Series(token_oov, name='token_oov')\n",
    "\n",
    "df = pd.concat([t, p, s, oov], axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuaiton or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatized_sentence_corpus(corpus):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    j = 0\n",
    "    i = 0\n",
    "    for parsed_article in nlp.pipe(corpus, \n",
    "                                   batch_size=50, n_threads=1):\n",
    "        \n",
    "        yield u' '.join([token.lemma_ for token in parsed_article\n",
    "                             if not punct_space(token)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for lemma in lemmatized_sentence_corpus(test_corpus):\n",
    "#    print(lemma)\n",
    "#    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"01/01/1998 Natel : Swisscom passe à l'offensive contre la protection des données L'opérateur national des télécommunications juge irréprochable son fichier sur les téléphones portables. Et accuse l'office chargé de la protection des données d'avoir cautionné un coup médiatique contre elle. L% entreprise Swisscom s'emploie depuis trois jours à désamorcer l' « afr faire des Natel ». Hier, l'opérateur unique (jusqu'au 31 décembre) des télécommunications en Suisse a défendu la légalité de son fichier sur les télé-phones portables, dont l'existence a été révélée dimanche. Swisscom accuse en outre l'office du préposé fédéral à la protection des données, Odilo Guntern, de « négligence » dans le traitement de l'affaire. Des représentants de Swisscom, de l'Office fédéral de la communication (OFCOM) et de la Police fédérale l'ont affirmé mardi : il n'y a pas de surveillance des mouvements des utilisateurs de Natel par Swisscom. Les conversations ne sont pas enregistrées et les autorités judiciaires n'ont accès qu'aux données qui sont stockées pour les besoins de l'entreprise. « Copies de sécurité » Seuls sont enregistrés pour six mois, comme le prévoit la loi, les numéros de l'appelant et de l'appelé, la centrale Natel concernée, ainsi que la date et l'heure de la communication lorsque celle-ci est établie. Ces données sont utilisées pour la facturation et la mise en évidence d'éventuelles erreurs du système. Des « copies de sécurité » des données sont conservées dans un « back up », d'où les autorités de poursuite pénale peuvent les tirer sur ordre d'un juge. « Toute l'affaire est lé-gale selon l'ancien comme le nouveau droit », a commenté Marc Furrer, directeur de l'OFCOM. Ce système « n'a rien à voir avec une protection préventive de l'Etat », a renchéri Jtirg Biihler, suppléant du chef de la Police fédérale. Un groupe de travail composé des autorités policières fédérales, de Swisscom et des autorités cantonales de poursuite pénale coordonne les surveillances téléphoniques officielles. Reste la question centrale : pourquoi le préposé fédéral à la protection des données, Odilo Guntern, ignorait-il l'existence de ce fichier, que Swisscom était pourtant tenu de déclarer ? Pour Jacques Bettex, porte-parole de l'entreprise, la réponse doit être cherchée du côté de M. Guntern, pas de Swisscom : « Nous pouvons prouver que ses services étaient informés de l'existence de ce fichier. Le sujet a été discuté il y a environ un mois, lors d'une séance de travail de l'Office de protection des données. » Pour Swisscom, il est dès lors extrêmement troublant qu'Odilo Guntern n'ait décidé d'ouvrir une enquête sur son fichier que lundi, après les révélations de la Sonntagszeitung. « Peut-être qu'il y a eu une négligence et que ses collaborateurs ont oublié de le mettre au courant Mais s'il avait pris la peine de se renseigner avant de répondre aux journalistes, jamais cette affaire n'aurait pris cette ampleur-là. M. Guntern est en train de perdre sa crédibilité dans cette histoire », accuse, très remonté, un cadre supérieur de Swisscom. Adjoint du préposé fédéral à la protection des données, Jean-Philippe Walter rétorque que l'annonce d'un fichier n'empêche pas qu'une enquête soit ouverte. M. Guntern poursuit donc son enquête et attend que Swisscom lui renvoie son questionnaire rempli d'ici le 9 janvier. Sy. B. avec ATS \""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[2990350]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
