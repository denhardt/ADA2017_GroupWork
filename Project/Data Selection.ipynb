{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import FrenchStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def month_dates(start, end):\n",
    "    f = lambda date: date.month + 12 * date.year\n",
    "\n",
    "    res = []\n",
    "    for tot_m in range(f(start)-1, f(end)):\n",
    "        y, m = divmod(tot_m, 12)\n",
    "        res.append(str(y) + '/' + '%02d' % (m+1))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_date(article):\n",
    "    \"\"\"\n",
    "    This method returns the date of the article\n",
    "    \"\"\"\n",
    "    str_date = article.find('entity').find('meta').find('issue_date').text\n",
    "    return datetime.strptime(str_date, '%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_articles_in_file(file, start_date, end_date):\n",
    "    articles = []  \n",
    "    for article in file.iter('article'):\n",
    "        if article.find('entity') is not None:\n",
    "            a = ''\n",
    "            date = get_date(article)\n",
    "            if start_date <= date <= end_date:\n",
    "                for entity in article.iter('entity'):\n",
    "                    a += entity.findtext('full_text') + ' '\n",
    "                articles.append(date.strftime('%d/%m/%Y') + ' ' + a)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_articles(path, start_date, end_date):\n",
    "    articles = []\n",
    "    for m_date in month_dates(start_date, end_date):\n",
    "        try:\n",
    "            file = etree.parse(path + m_date + '.xml')\n",
    "            articles.append(get_articles_in_file(file, start_date, end_date))\n",
    "        except (FileNotFoundError, IOError):\n",
    "            pass\n",
    "    return [a for file in articles for a in file]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/mbanga/Desktop/JDG/'\n",
    "start_date =  datetime(1990, 1, 1)\n",
    "end_date = datetime(1990, 1, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles = get_articles(path, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3434"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = fr_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fr_core_news_sm\n",
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuaiton or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_french(word):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens that\n",
    "    are not french words.\n",
    "    \"\"\"\n",
    "    d = enchant.Dict('fr_FR')\n",
    "    return d.check(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatized_sentence_corpus(corpus):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse reviews,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    j = 0\n",
    "    i = 0\n",
    "    for parsed_article in nlp.pipe(corpus, \n",
    "                                   batch_size=50, n_threads=1):\n",
    "        \n",
    "        date = parsed_article[0].text + ' '\n",
    "        yield date + u' '.join([token.lemma_ for token in parsed_article\n",
    "                             if not punct_space(token) and is_french(token.text) and not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/01/1990 le panama tremplin le panama jamais prendre sérieux voisin latin dans pays créer début siècle autour canal chapeau indépendance formel rester aléatoire largement américain dollar servir monnayer national troupe américain intervenir foi an la unième foi injustifiable plan droit international entourer série circonstance atténuant commencer régime inique subir isthme général il falloir ajouter renverser régime coup forcer prendre soin éviter victime civil américains accomplir boire principal ils ré surcroît reportage 10- reconnaissance le difficile matière militaire décrocher jour passer opération critiquer si partisan dictateur déposer regrouper harceler fragile autorité civil risquer lutte 1-drogue en pression courir nonciature lu mauvais grand oeil ion or curer hiérarchie combattre justement modernisation américanisation société accroissement injustice social dépourvu international panama apparaître géographiquement délicat articulation nord sud continent creuset rationalité us pragmatique conquérant transmuer partiellement sentiment homme visage grêler produire relation léonin géant nord annexer canal activiste informer largement fabriquer pentagone service renseignement la crapule international fait oeil américain commencer toucher argent droguer laver banque pana- ensuite recourir magie noir brésilien amitié cubain pression suivre refuge final bras maltraiter achever itinéraire emblématique latin fidèle vocation protecteur livrer vérité juridique dossier mais déjà opération servir tremplin objectif fondamental sous-continent répression trafic droguer panama cuba naguère ouvrir largement pister atterrissage avionnette charger cocaïne forts capturer récent 0 renversement unis vouloir pousser avantager ils envisager large escadre surveillance aérien intercepter transport au sud pays concerner gros bâton grand discourir américain passer agir schématiquement faire prix fort pays producteur côté consommation effort prévention entreprendre cette nouveau variation économie offrir imposer convaincre \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lem in lemmatized_sentence_corpus(articles[:1]):\n",
    "    print(lem, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "#import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV   False\n",
      "PUNCT   False\n",
      "PRON   True\n",
      "AUX   True\n",
      "VERB   False\n",
      "VERB   False\n",
      "DET   True\n",
      "NOUN   False\n",
      "ADP   True\n",
      "DET   True\n",
      "NOUN   False\n"
     ]
    }
   ],
   "source": [
    "text = ['Hier, je suis allé mangé des pommes avec les frères', \n",
    "        'Je ne sais pas quoi fera pour gamins violente']\n",
    "\n",
    "for token in nlp(text[0]):\n",
    "    print(token.pos_, ' ', token.is_stop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
