{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import FrenchStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def month_dates(start, end):\n",
    "    f = lambda date: date.month + 12 * date.year\n",
    "\n",
    "    res = []\n",
    "    for tot_m in range(f(start)-1, f(end)):\n",
    "        y, m = divmod(tot_m, 12)\n",
    "        res.append(str(y) + '/' + '%02d' % (m+1))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_date(article):\n",
    "    \"\"\"\n",
    "    This method returns the date of the article\n",
    "    \"\"\"\n",
    "    str_date = article.find('entity').find('meta').find('issue_date').text\n",
    "    return datetime.strptime(str_date, '%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_articles_in_file(file, start_date, end_date):\n",
    "    articles = []  \n",
    "    for article in file.iter('article'):\n",
    "        if article.find('entity') is not None:\n",
    "            a = ''\n",
    "            date = get_date(article)\n",
    "            if start_date <= date <= end_date:\n",
    "                for entity in article.iter('entity'):\n",
    "                    a += entity.findtext('full_text') + ' '\n",
    "                articles.append(date.strftime('%d/%m/%Y') + ' ' + a)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_articles(path, start_date, end_date):\n",
    "    articles = []\n",
    "    for m_date in month_dates(start_date, end_date):\n",
    "        try:\n",
    "            file = etree.parse(path + m_date + '.xml')\n",
    "            articles.append(get_articles_in_file(file, start_date, end_date))\n",
    "        except (FileNotFoundError, IOError):\n",
    "            pass\n",
    "    return [a for file in articles for a in file]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/home/mbanga/Desktop/JDG/'\n",
    "start_date =  datetime(1990, 1, 1)\n",
    "end_date = datetime(1990, 1, 31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "articles = get_articles(path, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3434"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = fr_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import fr_core_news_sm\n",
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuaiton or whitespace\n",
    "    \"\"\"\n",
    "    \n",
    "    return token.is_punct or token.is_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_french(word):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens that\n",
    "    are not french words.\n",
    "    \"\"\"\n",
    "    d = enchant.Dict('fr_FR')\n",
    "    return d.check(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatized_sentence_corpus(corpus):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse articles,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    j = 0\n",
    "    i = 0\n",
    "    for parsed_article in nlp.pipe(corpus, \n",
    "                                   batch_size=50, n_threads=1):\n",
    "        \n",
    "        date = parsed_article[0].text + ' '\n",
    "        yield date + u' '.join([token.lemma_ for token in parsed_article\n",
    "                             if not punct_space(token) and is_french(token.text) and not token.is_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01/01/1990 le panama tremplin le panama jamais prendre sérieux voisin latin dans pays créer début siècle autour canal chapeau indépendance formel rester aléatoire largement américain dollar servir monnayer national troupe américain intervenir foi an la unième foi injustifiable plan droit international entourer série circonstance atténuant commencer régime inique subir isthme général il falloir ajouter renverser régime coup forcer prendre soin éviter victime civil américains accomplir boire principal ils ré surcroît reportage 10- reconnaissance le difficile matière militaire décrocher jour passer opération critiquer si partisan dictateur déposer regrouper harceler fragile autorité civil risquer lutte 1-drogue en pression courir nonciature lu mauvais grand oeil ion or curer hiérarchie combattre justement modernisation américanisation société accroissement injustice social dépourvu international panama apparaître géographiquement délicat articulation nord sud continent creuset rationalité us pragmatique conquérant transmuer partiellement sentiment homme visage grêler produire relation léonin géant nord annexer canal activiste informer largement fabriquer pentagone service renseignement la crapule international fait oeil américain commencer toucher argent droguer laver banque pana- ensuite recourir magie noir brésilien amitié cubain pression suivre refuge final bras maltraiter achever itinéraire emblématique latin fidèle vocation protecteur livrer vérité juridique dossier mais déjà opération servir tremplin objectif fondamental sous-continent répression trafic droguer panama cuba naguère ouvrir largement pister atterrissage avionnette charger cocaïne forts capturer récent 0 renversement unis vouloir pousser avantager ils envisager large escadre surveillance aérien intercepter transport au sud pays concerner gros bâton grand discourir américain passer agir schématiquement faire prix fort pays producteur côté consommation effort prévention entreprendre cette nouveau variation économie offrir imposer convaincre \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lem in lemmatized_sentence_corpus(articles[:1]):\n",
    "    print(lem, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "#import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "don't know how to handle uri <gensim.models.phrases.Phrases object at 0x7f2f748b2c50>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/home/mbanga/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1611\u001b[0m             \u001b[0;31m# Things that don't have seek will trigger an exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1612\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1613\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Phrases' object has no attribute 'seek'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-157-7abf5440304d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#bigram = Phraser(Phrases(text))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mbigram_dico\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLineSentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdico\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigram_dico\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mbanga/anaconda3/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mbanga/anaconda3/lib/python3.6/site-packages/gensim/corpora/dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[0;34m(self, documents, prune_at)\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0munique\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mdocno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;31m# log progress & run a regular check for pruning, once every 10k docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdocno\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mbanga/anaconda3/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m             \u001b[0;31m# If it didn't work like a file, use it as a string filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmart_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m                     \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mbanga/anaconda3/lib/python3.6/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36msmart_open\u001b[0;34m(uri, mode, **kw)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'don\\'t know how to handle uri %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: don't know how to handle uri <gensim.models.phrases.Phrases object at 0x7f2f748b2c50>"
     ]
    }
   ],
   "source": [
    "text = ['Je', 'm', 'appelle', 'Pierre', 'Armel', 'Mbanga', 'Ndjock']\n",
    "#bigram = Phraser(Phrases(text))\n",
    "bigram = Phrases(text)\n",
    "bigram_dico = Dictionary(LineSentence(bigram))\n",
    "\n",
    "dico = Dictionary(bigram_dico)\n",
    "\n",
    "# learn the dictionnary by iterating over all of the articles\n",
    "#dico = Dictionary(articles[0].split())\n",
    "\n",
    "# filter tokens that are very rare or too common from\n",
    "# the dictionary \n",
    "dico.filter_extremes(no_below=10, no_above=0.4)\n",
    "\n",
    "# reassign integer lda\n",
    "dico.compactify()\n",
    "\n",
    "#path = os.path.join(path, 'Dictionary/')\n",
    "#trigram_dictionary.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADV   False\n",
      "PUNCT   False\n",
      "PRON   True\n",
      "AUX   True\n",
      "VERB   False\n",
      "VERB   False\n",
      "DET   True\n",
      "NOUN   False\n",
      "ADP   True\n",
      "DET   True\n",
      "NOUN   False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ['Hier, je suis allé mangé des pommes avec les frères', \n",
    "        'Je ne sais pas quoi fera pour gamins violente']\n",
    "\n",
    "for token in nlp(text[0]):\n",
    "    print(token.pos_, ' ', token.is_stop)\n",
    "\n",
    "#print('Pierre Armel Mbanga Ndjock'.decode('utf-8'))\n",
    "bigram.corpus_word_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
