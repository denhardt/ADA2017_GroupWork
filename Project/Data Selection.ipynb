{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Importing data\n",
    "For showing what we've done so far, the following parsing is only done on a fraction of the overall dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from lxml import etree\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = '/home/robin/GIT/ADA/ADA2017_GroupWork/Project/'\n",
    "# path to JDG .xml's\n",
    "path = '/home/robin/GIT/ADA/JDG/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining helper functions to be used for data parsing:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions are used to handle the data and extract the wanted articles from the \n",
    "'.xml'files and store them into an array of strings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes a start and an end date and retuns a list of all the months in between of the form mm/yyyy.\n",
    "def month_dates(start, end):\n",
    "    \"\"\"\n",
    "    Returns an array of strings giving the path\n",
    "    of which files to be processed\n",
    "    \"\"\"\n",
    "    f = lambda date: date.month + 12 * date.year\n",
    "\n",
    "    res = []\n",
    "    for tot_m in range(f(start)-1, f(end)):\n",
    "        y, m = divmod(tot_m, 12)\n",
    "        res.append(str(y) + '/' + '%02d' % (m+1))\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes an article as input and output the date of this articles in the format dd/mm/yyy. \n",
    "def get_date(article):\n",
    "    \"\"\"\n",
    "    This method returns the date of the article\n",
    "    \"\"\"\n",
    "    str_date = article.find('entity').find('meta').find('issue_date').text\n",
    "    return datetime.strptime(str_date, '%d/%m/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the articles in a '.xml' file and store the into an array of strings.\n",
    "def get_articles_in_file(file, start_date, end_date):\n",
    "    articles = []  \n",
    "    for article in file.iter('article'):\n",
    "        if article.find('entity') is not None:\n",
    "            a = ''\n",
    "            date = get_date(article)\n",
    "            if start_date <= date <= end_date:\n",
    "                for entity in article.iter('entity'):\n",
    "                    a += entity.findtext('full_text') + ' '\n",
    "                articles.append(date.strftime('%d/%m/%Y') + ' ' + a)\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using 'get_articles_in_file' goes throught all the '.xml' files in 'path' to store them in an array of strings.\n",
    "def get_articles(path, start_date, end_date):\n",
    "    articles = []\n",
    "    for m_date in month_dates(start_date, end_date):\n",
    "        try:\n",
    "            file = etree.parse(path + m_date + '.xml')\n",
    "            articles.append(get_articles_in_file(file, start_date, end_date))\n",
    "        except (FileNotFoundError, IOError):\n",
    "            pass\n",
    "    return [a for file in articles for a in file]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes as input a box_id and an '.xml' file and returns the text corresponding to box_id the the '.xml' file.\n",
    "def get_entity_text(file, box_id):\n",
    "    res = None\n",
    "    for article in file.iter('article'):\n",
    "        if article.find('entity') is not None:\n",
    "            date = get_date(article)\n",
    "            for entity in article.iter('entity'):\n",
    "                if   box_id == entity.find('meta').find('box').text:\n",
    "                    res = date.strftime('%d/%m/%Y') + ' ' + entity.findtext('full_text')\n",
    "                    break\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only parse a fraction of the data here, defined with `start_date` and `end_date`\n",
    "\n",
    "Parsing all files using previous helper functions and the lxml parser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date =  datetime(1990, 1, 1)\n",
    "end_date = datetime(1998, 2, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = get_articles(path, start_date, end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning and filtering\n",
    "### Lemmatisation\n",
    "We will be using the NLP library `spacy` to clean articles into more usable data. We'll be doing a lot of lemmatisation, whereby all words are reduced to a 'root' form, removing conjugation, pre- and suffixes and other such inflected forms. This will make it much easier to compare words between articles, as inflected forms will be transformed into the same lemma comparable across texts. For example, the words:\n",
    "\n",
    "> voter, votation, votons, vote, vot√©\n",
    "\n",
    "Will all be changed to the same 'root' word **voter**\n",
    "\n",
    "### Naive selection\n",
    "We next want to filter out articles which are not clearly in direct link to voting. Although we could have done a round of clustering to identify topics and then choose only those from the 'voting' cluster, we felt it better to use a combination of lemmatisation and 'naive selection', whereby articles are filtered depending on whether they contain any one of a list of keywords in relation to voting. The issue with performing clustering at this stage would be precision. We would then be performing two rounds of clustering, 1 to identify articles on voting, and another to identify the topics of these articles. We would rather sacrifice on recall and have higher precision early on so that we are sure that all selected articles are definitely on voting. Based on some initial trials, we found that it was an issue if non-voting articles made it to our later round of clustering, where it would throw off the clustering. Furthermore, the naive selection still left us with a large number of articles, several hundreds per year, which we decided was enough for our needs.\n",
    "\n",
    "At this point, we chose to do the naive selection before lemmatisation for performance reasons. We can surely increase the number of articles that we obtain without reducing precision if we perform the selection on lemmatised articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy # NLP library\n",
    "import fr_core_news_sm # Model to be used with spacy\n",
    "import enchant # spellchecking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = fr_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizations of the articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we got our articles we decided to lemmatize them before processing a classification algorithm on it so we could get better results.These are the functions we use for the lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to eliminate tokens that are pure punctuaiton or whitespace.\n",
    "def punct_space(token):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens\n",
    "    that are pure punctuaiton or whitespace\n",
    "    \"\"\"    \n",
    "    return token.is_punct or token.is_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to eliminate tokens that are not french words.\n",
    "def is_french(word):\n",
    "    \"\"\"\n",
    "    helper function to eliminate tokens that\n",
    "    are not french words.\n",
    "    \"\"\"\n",
    "    d = enchant.Dict('fr_FR')\n",
    "    return d.check(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator function to use spaCy to parse articles,lemmatize the text, and yield sentences.\n",
    "def lemmatized_corpus(corpus):\n",
    "    \"\"\"\n",
    "    generator function to use spaCy to parse articles,\n",
    "    lemmatize the text, and yield sentences\n",
    "    \"\"\"\n",
    "    pos = ['VERB', 'PROPN', 'NOUN', 'ADJ', 'ADV']\n",
    "    for parsed_article in nlp.pipe(corpus, \n",
    "                                   batch_size=100, n_threads=5):\n",
    "        # save the date\n",
    "        date = parsed_article[0].text\n",
    "        \n",
    "        yield (date, ' '.join([token.lemma_ for token in parsed_article if token.pos_ in pos]))\n",
    "                             \n",
    "        '''if not punct_space(token) and is_french(token.text)\n",
    "                                and not token.is_stop and not token.is_digit\n",
    "                                and not token.like_num]))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Na√Øve Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the fact that we a huge dataset of articles, We dicide to at first filter the articles using a simple selection by keywords.We initialize an array of string that are related to the tematic of the 'Votation', We might be losing some articles that would be meaningfull but regarding the size of our dataset we are ready to make this concession.We also think that it has some sence to do a keywords selection because it would hard to have an article about 'Votations' that does not cointains any word of our keywords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This fonction take an array of articles and a list of keywords 'lemmas' and returns all the articles that coitains one of more the words in our keywords list\n",
    "def corpus_votation(articles, lemmas):\n",
    "    votations = []\n",
    "    for article in articles:\n",
    "        if any(lemma in article for lemma in lemmas): \n",
    "            votations.append(article)\n",
    "    return votations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_votation_bis(articles, lemmas):\n",
    "    votations = []\n",
    "    for article in articles:\n",
    "        if any(lemma in article.replace(' ', '') for lemma in lemmas):\n",
    "            votations.append(article)\n",
    "    return votations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the naive selection part, where all articles that do not contain at least one of the keywords are filtered out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_selection(articles,keywords):\n",
    "    \n",
    "    articles_votation = []\n",
    "\n",
    "    for art in articles:\n",
    "        if any(word in art.split() for word in keywords):\n",
    "            articles_votation.append(art)\n",
    "    return articles_votation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive selection (First Filtering)\n",
    "lems = ['votation', 'referendum'] # todo: try adding 'initiative'\n",
    "\n",
    "#articles_votation_third = corpus_votation_bis(articles, lemmas)\n",
    "#articles_votation_bis = corpus_votation(articles, lemmas)\n",
    "\n",
    "articles_votation = article_selection(articles, lems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles_votation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here all articles are lemmatised, this step is relatively expensive in computation time. This could be parallelised in the future, by splitting up the set of articles into a certain number of groups and assigning one core to lemmatise each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization of the corpus of aticles selected using our naive selection and storing it in tuple of (dates,articles)\n",
    "if 0 == 1:\n",
    "    %%time\n",
    "    # Time consuming !!\n",
    "    lemmatized_corpus = [(date, lemmas) for date, lemmas in lemmatized_corpus(articles_votation)]\n",
    "\n",
    "    # retrieve dates\n",
    "    dates = [pair[0] for pair in lemmatized_corpus]\n",
    "\n",
    "    # retrieve articles\n",
    "    corpus = [pair[1] for pair in lemmatized_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dumping lemmatized articles to text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the articles we lemmatized before in '.txt' file.\n",
    "if 0 == 1:\n",
    "    with open(os.path.join(project_path, 'lemmatized articles 1990-1998.txt'), 'w') as file:\n",
    "        for article in corpus:\n",
    "            file.write(article + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the articles we lemmatized before in '.json' file.\n",
    "if 1 == 1:\n",
    "    with open(os.path.join(project_path, 'lemmatized articles 1990-1998.json'), 'w') as file:\n",
    "        json.dump(lemmatized_corpus, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(project_path, 'lemmatized articles 1990-1998.json'), 'r') as file:\n",
    "    lemmatized_corpus = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    # check ouput of lemmatizer (lemmatized_corpus) \n",
    "    file = etree.parse('/home/mbanga/Desktop/JDG/1990/01.xml')\n",
    "    box_id = '24 123 1446 2167'\n",
    "\n",
    "    original_text = [get_entity_text(file, box_id)]\n",
    "\n",
    "    for lemmatized in lemmatized_corpus(original_text):\n",
    "        print(lemmatized[1], '\\n')\n",
    "    print(original_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Unsupervised ML: LDA\n",
    "The next step is to identify the topics of all articles. This is a typical example of unsupervised ML whereby we want to assign labels in order to differentiate members (in this case articles) in a set of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0 == 1:\n",
    "    # check naive selection\n",
    "    file = etree.parse('/home/mbanga/Desktop/JDG/1990/01.xml')\n",
    "    box_id = '50 163 1090 888'\n",
    "\n",
    "    original_text = [get_entity_text(file, box_id)]\n",
    "    lemmas = ['vote', 'voter', 'votation', 'referendum']\n",
    "    res = corpus_votation(original_text, lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering articles about votations\n",
    "\n",
    "> Assumption: The subject of a votation is most likely to be found in\n",
    "the neighborhoud of the terms 'votation' or 'referendum' in the article. \n",
    "So we decided to extract the sentecence that cointais the keywords along with the sentences before and after.We consider that a sentence begins and end with a ',' which is usually the case but since the dataset that we have is not perfectly clean some errors occur collecting sentecens that are not really complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all phrases index with the searched term\n",
    "keywords = ['votation']\n",
    "#j = 0\n",
    "\n",
    "articles_votation_sents = []\n",
    "for article in articles_votation:\n",
    "    date = re.findall(r'^([^\\s]+)', article)[0]\n",
    "    #print('article', (j+1), ': ', date)\n",
    "    \n",
    "    sent = ''\n",
    "    phrases = article.split('.')    \n",
    "    for i, phrase in enumerate(phrases):\n",
    "        if any(keyword in phrase for keyword in keywords):\n",
    "            if len(phrases) < 2:\n",
    "                sent += phrase\n",
    "            elif i == 0:\n",
    "                sent += phrase[phrase.index(' ') + 1:] + ' '  + phrases[i+1]\n",
    "            elif i == len(phrases) - 1:\n",
    "                sent += ' ' + phrases[i-1] + ' ' + phrase\n",
    "            elif 0 < i < len(phrases) - 1:\n",
    "                sent += ' ' + phrases[i-1] + ' ' + phrase + ' ' + phrases[i+1]\n",
    "    articles_votation_sents.append(date + ' ' + sent)\n",
    "            #print(' {:}'.format(phrases[i-1] + phrase + phrases[i+1]))\n",
    "    #print('\\n')\n",
    "    #j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(articles_votation_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all phrases index with the searched term\n",
    "word = 'votation'\n",
    "j = 0\n",
    "for article in articles_votation[:10]:\n",
    "    date = re.findall(r'^([^\\s]+)', article)\n",
    "    print('article', (j+1), ': ', date)\n",
    "\n",
    "    phrases = article.split('.')\n",
    "    interest = []\n",
    "    for i, phrase in enumerate(phrases):\n",
    "        if word in phrase:\n",
    "            print(' {:}'.format(phrases[i-1] + phrase + phrases[i+1]))\n",
    "    print('\\n')\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "    %%time\n",
    "    # Time consuming !!\n",
    "    lemmatized_corpus = [(date, lemmas) for date, lemmas in lemmatized_corpus(articles_votation_sents)]\n",
    "\n",
    "    # retrieve dates\n",
    "    dates = [pair[0] for pair in lemmatized_corpus]\n",
    "\n",
    "    # retrieve articles\n",
    "    corpus = [pair[1] for pair in lemmatized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lemmatized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_corpus[501]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since all the articles that we got in our dataset are in french is was quite difficult to find a training dataset to fit a model that be able to classify our articles.We decide to use the Latent dirichlet allocation as our natural languge processing tool.Our aim was to minimize the bais of our topic classfication of the articles we exctracted.We could assign the mainstream votation topics(i.e army,economy,education...) and try to extract statics regarding a well defined set,but we did not want to make these kind of assumptions about the existance or the importance of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.corpora import Dictionary, MmCorpus\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "import warnings\n",
    "#import cPickle as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn the dictionnary by iterating over all of the articles\n",
    "dico = Dictionary([article.split() for article in corpus])\n",
    "\n",
    "# filter tokens that are very rare or too common from\n",
    "# the dictionary \n",
    "dico.filter_extremes(no_below=0, no_above=0.4)\n",
    "\n",
    "# reassign integer lda\n",
    "dico.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator function to read articles from a file and yield a bag-of-words representation.  \n",
    "def bow_generator(corpus):\n",
    "    \"\"\"\n",
    "    generator function to read articles from a file\n",
    "    and yield a bag-of-words representation\n",
    "    \"\"\"\n",
    "    for article in corpus:\n",
    "        yield dico.doc2bow(article.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate bag-of-word representations for\n",
    "# all reviews and save them as a matrix\n",
    "MmCorpus.serialize(os.path.join(project_path, 'corpus.mm'),\n",
    "                                bow_generator(corpus))\n",
    "\n",
    "bow_corpus = MmCorpus(os.path.join(project_path, 'corpus.mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storing our model\n",
    "lda_model_filepath = os.path.join(project_path, 'lda_model_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 1:\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "\n",
    "        # workers => sets the parallelism, and should be\n",
    "        # set to your number of physical cores minus one\n",
    "        lda = LdaMulticore(bow_corpus,\n",
    "                           num_topics=50,\n",
    "                           id2word=dico,\n",
    "                           workers=5)\n",
    "        \n",
    "        lda.save(lda_model_filepath)\n",
    "\n",
    "#load the finished LDA model from disk\n",
    "lda = LdaMulticore.load(lda_model_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accept a topic number and print out a formatted list of the top terms.\n",
    "def explore_topic(topic_number, topn=25):\n",
    "    \"\"\"\n",
    "    accept a user-supplied topic number and\n",
    "    print out a formatted list of the top terms\n",
    "    \"\"\"\n",
    "    \n",
    "    print(u'{:20} {}'.format(u'term', u'frequency') + u'\\n')\n",
    "    \n",
    "    for term, frequency in lda.show_topic(topic_number, topn):\n",
    "        print(u'{:20} {:.3f}'.format(term, round(frequency, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_topic(topic_number=15, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal is to find all documents related to the same topic\n",
    "def articles_topic(lda, bow_corpus, corpus, topic):\n",
    "    \"\"\"\n",
    "    return the list of articles associated\n",
    "    with a given topic.\n",
    "    \"\"\"\n",
    "    assert len(bow_corpus) == len(corpus)\n",
    "    nb_topics = len(lda.get_topics())\n",
    "    \n",
    "    documents = []\n",
    "    if 0 <= topic < nb_topics:\n",
    "        k = 0\n",
    "        for bow_article in bow_corpus:\n",
    "            dist = lda.get_document_topics(bow_article, minimum_probability=0)\n",
    "            dist = [p[1] for p in dist]\n",
    "            idx_max = dist.index(max(dist))\n",
    "            if idx_max == topic:\n",
    "                documents.append(corpus[k])\n",
    "            k += 1\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = articles_topic(lda, bow_corpus, articles_votation_sents, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1 == 1:     \n",
    "        LDAvis_prepared = pyLDAvis.gensim.prepare(lda, bow_corpus, dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(LDAvis_prepared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research questions and updated goals\n",
    "As we have gotten to grips with the data and started processing it we now have a clearer idea of how we are going to answer our original research questions, as well as modifications of our goals and methods\n",
    "\n",
    "> Do we have the freedom to vote on any question we wish?\n",
    "\n",
    "In retrospect this question is very general, and serves more as an introduction to the more specific subsequent questions we ask. It doesn't have a clear yes or no answer, but is most closely related to the question concerning direct democracy.\n",
    "\n",
    "> Do the same votations, or topics, keep coming up? Is this throughout history or during specific periods?\n",
    "\n",
    "To answer this question we would first perform LDA on our entire set filtered with our votation keywords. We would then compare date distributions of articles in different clusters. By seeing these distributions, we should be able to determine if certain topics were only being voted on at a certain point in the last 200 years, or if they regularly re-appear.\n",
    "\n",
    ">Are the results of these repeated votations changing throughout time?\n",
    "\n",
    "Given the work load and time pressure we have decided to drop this question. Accurately answering this question would require several additional layers of analysis. We would first have to identify the actual votations that potentially multiple articles refer to. Finding an automatic way to categorise whether a vote is accepted or not is another totally independent challenge too.\n",
    "\n",
    ">Can we link big changes in technological and societal norms to previous votations?\n",
    "\n",
    "The best way we have thought of to tackle this question involves first choosing several important changes in technology and society. We may then try to correlate these technology and society changes to big changes in the topics of votations, for example by seeing if a new votation topic concerning cars and roads appeared around the same time as the first industrially produced affordable cars.\n",
    "\n",
    ">How has direct democracy been used? Is it increasing or decreasing? What kind of votations is it used for?\n",
    "\n",
    "To answer this question we plan to perform LDA on our entire dataset to get an initial view of votation topics. Then we would compare this with LDA on a set of our data which is filtered with keywords relating only to the direct-democracy 'initiatives populaires'. We may first simply compare the size of these two sets over time, seeing if there are more or less 'initiatives populaires' throughout time. By comparing the topics that may be missing or not as represented in one set we may also draw some conclusions as to how direct democracy has been used, specifically on which topics or area of topics.\n",
    "\n",
    "### dropped goals\n",
    "* Increasing our data with the swiss tweets dataset. Again, given the time pressure we think it is best to focus on the news set first and foremost.\n",
    "* Votation results. As discussed above, due to the more complex nature of this question we think it better to be dropped.\n",
    "* Spark and cluster processing. As our initial dataset is not of a huge size, and that we discard a large proportion of it when we perform our naive filtering, we decided that the added overhead of learning spark wasn't worth it."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
