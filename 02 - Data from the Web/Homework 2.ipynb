{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import re\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Scraping info from topuniversities.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_url = 'https://www.topuniversities.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial postman/parsing\n",
    "Trying to get the url which contains the actual data that we want to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(qs_url + '/university-rankings/world-university-rankings/2018')\n",
    "soup = BeautifulSoup(resp.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts = soup.find_all('script', type='text/javascript')\n",
    "len(scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for script in scripts:\n",
    "    if script.text.find('rank_url')!= -1:\n",
    "        print(i)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts[28].text[58:]\n",
    "len(scripts[28].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts[28].text.find('rank_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts[28].text[14778:15178]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to parse headers to a python dict?\n",
    "resp.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual ranking data which is shown on the page is generated with a request to `rank_url`, therefore it is this\n",
    "linke that we'll need to GET to extract all the data we're interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the main data and putting everything into a `DataFrame`\n",
    "Scraping everything that is contained in the `rank_url`. This is the majority of what we are interested in, the faculty and student data are contained on another page that is specific to each university. This will be scraped afterwards.\n",
    "Handily enough, the data from `rank_url` is in `JSON` format, so we'll use the `JSON` parsing capabilities of \n",
    "`requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_url = 'https://www.topuniversities.com/sites/default/files/qs-rankings-data/357051.txt'\n",
    "rank_data = requests.get(rank_url)\n",
    "parsed_data = rank_data.json()\n",
    "parsed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a a `Dict` with only one key, so let's have a look into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too surprisingly the list we get in the `data` key is conveniently organised from highest to lowest ranked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(parsed_data) # dict\n",
    "type(parsed_data['data']) #list\n",
    "# List is already organised based on rank (with indexing starting at 0):\n",
    "parsed_data['data'][3]['rank_display']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data['data'][197]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now put all of this data into a single `DataFrame`. We're only interested in the top 200 universities, so we'll ignore the rest of the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_data = pd.DataFrame()\n",
    "for i in range(0,200):\n",
    "    qs_data = qs_data.append(parsed_data['data'][i], ignore_index=True)\n",
    "print(qs_data.shape)\n",
    "qs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the specific page for each university\n",
    "\n",
    "We will first define a handy little function to extract numbers from strings with newlines and commas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xtract_number(str):\n",
    "    str = str.replace(',' , '')\n",
    "    str = re.search(r'\\d+', str).group()\n",
    "    return str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the extra columns that we're going to populate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add = ['total faculty','inter faculty','total student','total inter']\n",
    "qs_data = pd.concat([qs_data, pd.DataFrame(columns=columns_to_add)], axis=1)\n",
    "qs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional information is contained in the following tags\n",
    "<h3> Number of international students\n",
    "<h3> Number of students\n",
    "<h3> Number of academic faculty staff --> <div class=\"anno\">In total & <div class=\"anno\">International"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step is very slow, it has to parse a lot of html for 200 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for idx in qs_data.index:\n",
    "for idx in range(0,11):\n",
    "    page = requests.get(qs_url + qs_data.loc[idx]['url'])\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    for column in columns_to_add:\n",
    "        try:\n",
    "            wrapper = soup.find_all('div',class_=column)\n",
    "            value = xtract_number(wrapper[1].find('div', class_='number').string)\n",
    "            qs_data.loc[idx][column] = value\n",
    "        except IndexError:\n",
    "            print('No data for', qs_data.loc[idx]['title'], 'concerning', column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can see that some data is missing for New York University and the Indian Institute of Science.\n",
    "Going to the website and checking this by hand does indeed show that these pieces of information are missing. We'll therefore leave these as NaN to signify the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_data.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( qs_data, open( \"qs_dataframe.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df = pickle.load( open( \"qs_dataframe.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Scraping top 200 universities from Times Higher Education\n",
    "\n",
    "We have the same issue as before, the HTML from the main page given doesn't contain the data that we actually\n",
    "want, rather it is loaded with a jQuery from a `json` somewhere else on the site. Using Postman and inspecting the html, there is only one `json` loaded on the ranking page, so we'll simply do some string handling to extract\n",
    "the url of interest from the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_url = 'https://www.timeshighereducation.com/world-university-rankings/2018/world-ranking'\n",
    "resp = requests.get(times_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract the url we want, we're first going to find where the \"json\" at the end of the url is located. We'll then use `rfind` to find the \"http\" at the beginning of this url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = resp.text.find('json')\n",
    "start = resp.text.rfind('http', 0 , stop)\n",
    "times_data_url = resp.text[start:stop+len('json')]\n",
    "print(times_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've still got all the escape characters, in this case backslashes, so we'll have to replace them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_data_url = times_data_url.replace('\\\\' , '')\n",
    "print(times_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_data = requests.get(times_data_url)\n",
    "#times_parsed = rank_data.json()\n",
    "#parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_data.text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_parsed = times_data.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got some other keys than just the `data` one, but they don't seem of use for what we're looking for. `location` is already contained in the main `data` key-value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_parsed.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it looks like `list` we get in the `data` key is conveniently organised from highest to lowest ranked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_parsed['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df = pd.DataFrame()\n",
    "for i in range(0,200):\n",
    "    times_df = times_df.append(times_parsed['data'][i], ignore_index=True)\n",
    "print(times_df.shape)\n",
    "times_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df.columnsmns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
