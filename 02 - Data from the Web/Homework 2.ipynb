{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import re\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Scraping info from topuniversities.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_url = 'https://www.topuniversities.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial postman/parsing\n",
    "Trying to get the url which contains the actual data that we want to parse. Using Postman we can see that the actual ranking data which is shown on the page is generated with a request to `rank_url`, therefore it is this\n",
    "link that we'll need to GET to extract all the data we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(qs_url + '/university-rankings/world-university-rankings/2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = resp.text.find('rank_url')\n",
    "start = resp.text.find('http', start)\n",
    "stop = resp.text.find('.txt', start)\n",
    "qs_data_url = resp.text[start:stop+len('.txt')]\n",
    "print(qs_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've still got all the escape characters, in this case backslashes, so we'll have to replace them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_data_url = qs_data_url.replace('\\\\' , '')\n",
    "print(qs_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the main data and putting everything into a `DataFrame`\n",
    "Scraping everything that is contained in the `rank_url`. This is the majority of what we are interested in, the faculty and student data are contained on another page that is specific to each university. This will be scraped afterwards.\n",
    "Handily enough, the data from `rank_url` is in `JSON` format, so we'll use the `JSON` parsing capabilities of \n",
    "`requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_url = 'https://www.topuniversities.com/sites/default/files/qs-rankings-data/357051.txt'\n",
    "rank_data = requests.get(qs_data_url)\n",
    "parsed_data = rank_data.json()\n",
    "parsed_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a `Dict` with only one key, so let's have a look into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(parsed_data['data']))\n",
    "print(len(parsed_data['data']))\n",
    "parsed_data['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a list of 959 entries. Not too surprisingly the list we get in the `data` key is conveniently organised from highest to lowest ranked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now put all of this data into a single `DataFrame`. We're only interested in the top 200 universities, so we'll ignore the rest of the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df = pd.DataFrame()\n",
    "for i in range(0,200):\n",
    "    qs_df = qs_df.append(parsed_data['data'][i], ignore_index=True)\n",
    "print(qs_df.shape)\n",
    "qs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now got a `DataFrame` with the main information, but we still need to scrape a page for each individual university that contains the number of total and international, faculty and students\n",
    "\n",
    "## Scraping the specific page for each university\n",
    "\n",
    "We will first define a handy little function to extract numbers from strings with newlines and commas, for example from `\\n1,300` we want to extract only the `1300`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xtract_number(str_in):\n",
    "    str_in = str_in.replace(',' , '')\n",
    "    str_in = re.search(r'\\d+', str_in).group()\n",
    "    return str_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the extra columns that we're going to populate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add = ['total faculty','inter faculty','total student','total inter']\n",
    "qs_df = pd.concat([qs_df, pd.DataFrame(columns=columns_to_add)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional information is contained in the following tags\n",
    "#<h3> Number of international students\n",
    "#<h3> Number of students\n",
    "#<h3> Number of academic faculty staff --> <div class=\"anno\">In total & <div class=\"anno\">International"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page containing the additional data we're looking for is already contained in the `url` field of our `DataFrame`. Each value we're looking for is contained in a `<div>` tag with `class=` the data we're looking for, within this tag is another `<div>` tag with `class=\"number\"` which has the actual numeric value. We're therefore going to parse the page for each university and use `BeautifulSoup` to find all these tags. As there are several of them on each page, we'll double-check that they're  all the same.\n",
    "The following step is very slow, it has to parse a lot of html for 200 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in qs_df.index:\n",
    "#for idx in range(30,200): # you can uncomment this to only parse the first few universities\n",
    "    page = requests.get(qs_url + qs_df.loc[idx]['url']) # GET the page for one university\n",
    "    soup = BeautifulSoup(page.text, 'html.parser') # parse it with bs4\n",
    "\n",
    "    for column in columns_to_add:\n",
    "        try:\n",
    "            wrapper = soup.find_all('div', class_=column) # find the tag of interest\n",
    "            if not wrapper:\n",
    "                print('No data for', qs_df.loc[idx]['title'], 'concerning', column)\n",
    "            values = np.zeros(len(wrapper))\n",
    "            for i in range(0,len(wrapper)): # if there are several tags, we'll check they have the same values\n",
    "                values[i] = xtract_number(wrapper[0].find('div', class_='number').string)\n",
    "                if i>0 and values[i] != values[i-1]:\n",
    "                   raise Exception('Numerical values for', qs_df.loc[idx]['title'], 'are different throughout the HTML') \n",
    "                else:\n",
    "                    qs_df.loc[idx][column] = values[0]\n",
    "            \n",
    "        except IndexError:\n",
    "            print('No data for', qs_df.loc[idx]['title'], 'concerning', column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can see that some data is missing for New York University and the Indian Institute of Science.\n",
    "Going to the website and checking this by hand does indeed show that these pieces of information are missing. We'll therefore leave these as NaN to signify the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the time to GET and parse all this HTML, we've stored the `DataFrame` in a pickle for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump( qs_df, open( \"qs_dataframe.p\", \"wb\" ) )\n",
    "#qs_df = pickle.load( open( \"qs_dataframe.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop some of the extra columns that we don't really need, they're still in the pickle if we need them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df.drop(['core_id', 'guide', 'logo', 'nid', 'url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which are the best universities?\n",
    "\n",
    "We are now going to compare the best universities in terms of ratio between faculty members: students and % of international students. Let's add these columns, they are merely operations involving the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df['faculty:students ratio'] = qs_df['total faculty']/qs_df['total student']\n",
    "qs_df['% international students'] = 100*qs_df['total inter']/qs_df['total student']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df.sort_values('faculty:students ratio' , ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df.sort_values('% international students' , ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating by Country\n",
    "\n",
    "We'll first create a new `DataFrame` which will have info aggregated by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_country = pd.DataFrame(columns=['Country'] + columns_to_add )\n",
    "qs_country['Country'] = qs_df['country'].unique()\n",
    "qs_country.set_index('Country', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the totals per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for country in qs_df['country'].unique():\n",
    "    sums = qs_df[qs_df['country'] == country][columns_to_add].sum()\n",
    "    qs_country.loc[country][columns_to_add] = sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the same two stats that we did per university before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_country['faculty:students ratio'] = qs_country['total faculty']/qs_country['total student']\n",
    "qs_country['% international students'] = 100*qs_country['total inter']/qs_country['total student']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_country.sort_values('faculty:students ratio' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_country.sort_values('% international students' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating by region\n",
    "Let's do the same thing but grouping per region now, we'll do this in the same way as just before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_region = pd.DataFrame(columns=['Region'] + columns_to_add )\n",
    "qs_region['Region'] = qs_df['region'].unique()\n",
    "qs_region.set_index('Region', inplace=True)\n",
    "qs_region.head()\n",
    "\n",
    "for region in qs_df['region'].unique():\n",
    "    sums = qs_df[qs_df['region'] == region][columns_to_add].sum()\n",
    "    qs_region.loc[region][columns_to_add] = sums\n",
    "    \n",
    "qs_region['faculty:students ratio'] = qs_region['total faculty']/qs_region['total student']\n",
    "qs_region['% international students'] = 100*qs_region['total inter']/qs_region['total student']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_region.sort_values('faculty:students ratio' , ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_region.sort_values('% international students' , ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Scraping top 200 universities from Times Higher Education\n",
    "\n",
    "We have the same issue as before, the HTML from the given url doesn't contain the data that we actually\n",
    "want, rather it is loaded with a jQuery to a `json` somewhere else on the site. Using Postman and inspecting the html, there is only one `json` loaded on the ranking page, so we'll simply do some string handling to extract\n",
    "the url of interest from the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_url = 'https://www.timeshighereducation.com/world-university-rankings/2018/world-ranking'\n",
    "resp = requests.get(times_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract the url we want, we're first going to find where the \"json\" at the end of the url is located. We'll then use `rfind` to find the \"http\" at the beginning of this url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = resp.text.find('json')\n",
    "start = resp.text.rfind('http', 0 , stop)\n",
    "times_data_url = resp.text[start:stop+len('json')]\n",
    "print(times_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to filter out all the backlashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_data_url = times_data_url.replace('\\\\' , '')\n",
    "print(times_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_data = requests.get(times_data_url)\n",
    "times_parsed = times_data.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got some other keys than just the `data` one, but they don't seem of use for what we're looking for. `location` is already contained in the main `data` key-value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_parsed.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it looks like `list` we get in the `data` key is conveniently organised from highest to lowest ranked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_parsed['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our `DataFrame` containing the top 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df = pd.DataFrame()\n",
    "for i in range(0,200):\n",
    "    times_df = times_df.append(times_parsed['data'][i], ignore_index=True)\n",
    "print(times_df.shape)\n",
    "times_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a lot more information this time, let's get ride of the columns we're not interested in, after backing it up to a pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( times_df, open( \"times_dataframe.p\", \"wb\" ) )\n",
    "times_df = times_df[['location','name','rank','stats_student_staff_ratio','stats_number_students','stats_pc_intl_students']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add region information based on what we have from the previous `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df['Region'] = np.nan\n",
    "for country in times_df['location'].unique():\n",
    "    try:\n",
    "        times_df.loc[times_df['location'] == country, 'Region'] = qs_df[qs_df['country'] == country]['region'].iloc[0]\n",
    "    except IndexError:\n",
    "        print('No region info for', country)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df = pickle.load( open( \"times_dataframe.p\", \"rb\" ) )\n",
    "times_df = times_df[['location','name','rank','stats_student_staff_ratio','stats_number_students','stats_pc_intl_students']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're missing some region info about Luxembourg and the Russian Federation, so we'll add this by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df.loc[times_df['location'] == 'Luxembourg', 'Region'] = 'Europe'\n",
    "times_df.loc[times_df['location'] == 'Russian Federation', 'Region'] = 'Russia'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best universities\n",
    "\n",
    "We'll change types to floats where we need it. We also need to do a little bit of string cleaning before handing it over to `Pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df.loc[:,'stats_number_students'] = times_df.loc[:,'stats_number_students'].str.replace(',' , '')\n",
    "times_df.loc[:,'stats_pc_intl_students'] = times_df.loc[:,'stats_pc_intl_students'].str.replace('%' , '')\n",
    "\n",
    "columns = ['stats_student_staff_ratio', 'stats_number_students', 'stats_pc_intl_students']\n",
    "for col in columns:\n",
    "    times_df.loc[:,col] = times_df.loc[:,col].astype(float)\n",
    "times_df['faculty:students ratio'] = 1/times_df['stats_student_staff_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df.sort_values('faculty:students ratio' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df.sort_values('stats_pc_intl_students' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've only got the % international students and students:staff ratio in the data from the Times, so we'll calculate the number of staff and international students from this data. Note that we don't have any info concerning % international faculty from the Times data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df['total inter'] = times_df['stats_number_students']*times_df['stats_pc_intl_students']/100\n",
    "times_df['total inter'] = times_df['total inter'].astype(int)\n",
    "times_df['total faculty'] = times_df['stats_number_students']/times_df['stats_student_staff_ratio']\n",
    "times_df['total faculty'] = times_df['total faculty'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add = ['total faculty', 'stats_number_students', 'total inter']\n",
    "times_country = pd.DataFrame(columns=['Country'] + columns_to_add )\n",
    "times_country['Country'] = times_df['location'].unique()\n",
    "times_country.set_index('Country', inplace=True)\n",
    "\n",
    "for country in times_df['location'].unique():\n",
    "    sums = times_df[times_df['location'] == country][columns_to_add].sum()\n",
    "    times_country.loc[country][columns_to_add] = sums\n",
    "\n",
    "times_country['faculty:students ratio'] = times_country['total faculty']/times_country['stats_number_students']\n",
    "times_country['% international students'] = 100*times_country['total inter']/times_country['stats_number_students']\n",
    "times_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_country.sort_values('faculty:students ratio' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_country.sort_values('% international students' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add = ['total faculty', 'stats_number_students', 'total inter']\n",
    "times_region = pd.DataFrame(columns=['Region'] + columns_to_add )\n",
    "times_region['Region'] = times_df['Region'].unique()\n",
    "times_region.set_index('Region', inplace=True)\n",
    "\n",
    "for region in times_df['Region'].unique():\n",
    "    sums = times_df[times_df['Region'] == region][columns_to_add].sum()\n",
    "    times_region.loc[region, columns_to_add] = sums\n",
    "    \n",
    "times_region['faculty:students ratio'] = times_region['total faculty']/times_region['stats_number_students']\n",
    "times_region['% international students'] = 100*times_region['total inter']/times_region['stats_number_students']\n",
    "times_region.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_region.sort_values('faculty:students ratio' , ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_region.sort_values('% international students' , ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Merging both `DataFrames`\n",
    "\n",
    "As we've been looking at the number of students and faculty as well as their origin, we'll only keep this data in the merged `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df[qs_df['country'].str.contains('Russia')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df['country'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(times_df['name'].unique()).intersection(qs_df['title'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df.rename(columns={'title':'name'}, inplace=True)\n",
    "qs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should do the merge after changing the names to get most similar\n",
    "mrg_df = times_df.merge(qs_df, how='inner', on='name')\n",
    "print(mrg_df.shape)\n",
    "mrg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff1 = list(set(times_df['name'].unique()).difference(qs_df['name'].unique()))\n",
    "diff2 = list(set(qs_df['name'].unique()).difference(times_df['name'].unique()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in diff1:\n",
    "    if len(qs_df[qs_df['name'].str.contains(i)]) > 0:\n",
    "        qs_df.loc[qs_df['name'].str.contains(i),'name'] = i\n",
    "for i in diff2:\n",
    "    if len(times_df[times_df['name'].str.contains(i)]) > 0:\n",
    "        times_df.loc[times_df['name'].str.contains(i),'name'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df.loc[qs_df['name'].str.contains('Exeter'),'name'].str"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
