{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import re\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Scraping info from topuniversities.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_url = 'https://www.topuniversities.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial postman/parsing\n",
    "Trying to get the url which contains the actual data that we want to parse. Using Postman we can see that the actual ranking data which is shown on the page is generated with a request to `rank_url`, therefore it is this\n",
    "link that we'll need to GET to extract all the data we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(qs_url + '/university-rankings/world-university-rankings/2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = resp.text.find('rank_url')\n",
    "start = resp.text.find('http', start)\n",
    "stop = resp.text.find('.txt', start)\n",
    "qs_data_url = resp.text[start:stop+len('.txt')]\n",
    "print(qs_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've still got all the escape characters, in this case backslashes, so we'll have to replace them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_data_url = qs_data_url.replace('\\\\' , '')\n",
    "print(qs_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the main data and putting everything into a `DataFrame`\n",
    "Scraping everything that is contained in the `rank_url`. This is the majority of what we are interested in, the faculty and student data are contained on another page that is specific to each university. This will be scraped afterwards.\n",
    "Handily enough, the data from `rank_url` is in `JSON` format, so we'll use the `JSON` parsing capabilities of \n",
    "`requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_url = 'https://www.topuniversities.com/sites/default/files/qs-rankings-data/357051.txt'\n",
    "rank_data = requests.get(qs_data_url)\n",
    "parsed_data = rank_data.json()\n",
    "parsed_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a `Dict` with only one key, so let's have a look into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(parsed_data['data']))\n",
    "print(len(parsed_data['data']))\n",
    "parsed_data['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a list of 959 entries. Not too surprisingly the list we get in the `data` key is conveniently organised from highest to lowest ranked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now put all of this data into a single `DataFrame`. We're only interested in the top 200 universities, so we'll ignore the rest of the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df = pd.DataFrame()\n",
    "for i in range(0,200):\n",
    "    qs_df = qs_df.append(parsed_data['data'][i], ignore_index=True)\n",
    "print(qs_df.shape)\n",
    "qs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now got a `DataFrame` with the main information, but we still need to scrape a page for each individual university that contains the number of total and international, faculty and students\n",
    "\n",
    "## Scraping the specific page for each university\n",
    "\n",
    "We will first define a handy little function to extract numbers from strings with newlines and commas, for example from `\\n1,300` we want to extract only the `1300`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xtract_number(str_in):\n",
    "    str_in = str_in.replace(',' , '')\n",
    "    str_in = re.search(r'\\d+', str_in).group()\n",
    "    return str_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the extra columns that we're going to populate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add = ['total faculty','inter faculty','total student','total inter']\n",
    "qs_df = pd.concat([qs_df, pd.DataFrame(columns=columns_to_add)], axis=1)\n",
    "qs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Additional information is contained in the following tags\n",
    "#<h3> Number of international students\n",
    "#<h3> Number of students\n",
    "#<h3> Number of academic faculty staff --> <div class=\"anno\">In total & <div class=\"anno\">International"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page containing the additional data we're looking for is already contained in the `url` field of our `DataFrame`. Each value we're looking for is contained in a `<div>` tag with `class=` the data we're looking for, within this tag is another `<div>` tag with `class=\"number\"` which has the actual numeric value. We're therefore going to parse the page for each university and use `BeautifulSoup` to find all these tags. As there are several of them on each page, we'll double-check that they're  all the same.\n",
    "The following step is very slow, it has to parse a lot of html for 200 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in qs_df.index:\n",
    "#for idx in range(30,200): # you can uncomment this to only parse the first few universities\n",
    "    page = requests.get(qs_url + qs_df.loc[idx]['url']) # GET the page for one university\n",
    "    soup = BeautifulSoup(page.text, 'html.parser') # parse it with bs4\n",
    "\n",
    "    for column in columns_to_add:\n",
    "        try:\n",
    "            wrapper = soup.find_all('div', class_=column) # find the tag of interest\n",
    "            if not wrapper:\n",
    "                print('No data for', qs_df.loc[idx]['title'], 'concerning', column)\n",
    "            values = np.zeros(len(wrapper))\n",
    "            for i in range(0,len(wrapper)): # if there are several tags, we'll check they have the same values\n",
    "                values[i] = xtract_number(wrapper[0].find('div', class_='number').string)\n",
    "                if i>0 and values[i] != values[i-1]:\n",
    "                   raise Exception('Numerical values for', qs_df.loc[idx]['title'], 'are different throughout the HTML') \n",
    "                else:\n",
    "                    qs_df.loc[idx][column] = values[0]\n",
    "            \n",
    "        except IndexError:\n",
    "            print('No data for', qs_df.loc[idx]['title'], 'concerning', column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can see that some data is missing for New York University and the Indian Institute of Science.\n",
    "Going to the website and checking this by hand does indeed show that these pieces of information are missing. We'll therefore leave these as NaN to signify the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the time to GET and parse all this HTML, we've stored the `DataFrame` in a pickle for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.dump( qs_df, open( \"qs_dataframe.p\", \"wb\" ) )\n",
    "#qs_df = pickle.load( open( \"qs_dataframe.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop some of the extra columns that we don't really need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df.drop(['core_id', 'guide', 'logo', 'nid', 'url'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which are the best universities?\n",
    "\n",
    "We are now going to compare the best universities in terms of ratio between faculty members: students and % of international students. Let's add these columns, they are merely operations involving the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df['faculty:students ratio'] = qs_df['total faculty']/qs_df['total student']\n",
    "qs_df['% international students'] = 100*qs_df['total inter']/qs_df['total student']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df.sort_values('faculty:students ratio' , ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df.sort_values('% international students' , ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_df['country'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Scraping top 200 universities from Times Higher Education\n",
    "\n",
    "We have the same issue as before, the HTML from the main page given doesn't contain the data that we actually\n",
    "want, rather it is loaded with a jQuery to a `json` somewhere else on the site. Using Postman and inspecting the html, there is only one `json` loaded on the ranking page, so we'll simply do some string handling to extract\n",
    "the url of interest from the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_url = 'https://www.timeshighereducation.com/world-university-rankings/2018/world-ranking'\n",
    "resp = requests.get(times_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract the url we want, we're first going to find where the \"json\" at the end of the url is located. We'll then use `rfind` to find the \"http\" at the beginning of this url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = resp.text.find('json')\n",
    "start = resp.text.rfind('http', 0 , stop)\n",
    "times_data_url = resp.text[start:stop+len('json')]\n",
    "print(times_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to filter out all the backlashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_data_url = times_data_url.replace('\\\\' , '')\n",
    "print(times_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_data = requests.get(times_data_url)\n",
    "#times_parsed = rank_data.json()\n",
    "#parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_data.text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_parsed = times_data.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got some other keys than just the `data` one, but they don't seem of use for what we're looking for. `location` is already contained in the main `data` key-value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_parsed.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it looks like `list` we get in the `data` key is conveniently organised from highest to lowest ranked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_parsed['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df = pd.DataFrame()\n",
    "for i in range(0,200):\n",
    "    times_df = times_df.append(times_parsed['data'][i], ignore_index=True)\n",
    "print(times_df.shape)\n",
    "times_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times_df.columns"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
