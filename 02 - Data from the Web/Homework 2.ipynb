{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import re\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Scraping info from topuniversities.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_url = 'https://www.topuniversities.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial postman/parsing\n",
    "Trying to get the url which contains the actual data that we want to parse. Using Postman we can see that the actual ranking data which is shown on the page is generated with a request to `rank_url`, therefore it is this\n",
    "link that we'll need to GET to extract all the data we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resp = requests.get(qs_url + '/university-rankings/world-university-rankings/2018')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = resp.text.find('rank_url')\n",
    "start = resp.text.find('http', start)\n",
    "stop = resp.text.find('.txt', start)\n",
    "qs_data_url = resp.text[start:stop+len('.txt')]\n",
    "print(qs_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've still got all the escape characters, in this case backslashes, so we'll have to replace them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_data_url = qs_data_url.replace('\\\\' , '')\n",
    "print(qs_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the main data and putting everything into a `DataFrame`\n",
    "Scraping everything that is contained in the `rank_url`. This is the majority of what we are interested in, the faculty and student data are contained on another page that is specific to each university. This will be scraped afterwards.\n",
    "Handily enough, the data from `rank_url` is in `JSON` format, so we'll use the `JSON` parsing capabilities of \n",
    "`requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rank_data = requests.get(qs_data_url)\n",
    "parsed_data = rank_data.json()\n",
    "parsed_data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a `Dict` with only one key, so let's have a look into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(parsed_data['data']))\n",
    "print(len(parsed_data['data']))\n",
    "parsed_data['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a list of 959 entries. Not too surprisingly the list we get in the `data` key is conveniently organised from highest to lowest ranked."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now put all of this data into a single `DataFrame`. We're only interested in the top 200 universities, so we'll ignore the rest of the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_df = pd.DataFrame()\n",
    "for i in range(0,200):\n",
    "    qs_df = qs_df.append(parsed_data['data'][i], ignore_index=True)\n",
    "print(qs_df.shape)\n",
    "qs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now got a `DataFrame` with the main information, but we still need to scrape a page for each individual university that contains the number of total and international, faculty and students.\n",
    "\n",
    "## Scraping the specific page for each university\n",
    "\n",
    "We will first define a handy little function to extract numbers from strings with newlines and commas, for example from `\\n1,300` we want to extract only the `1300`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def xtract_number(str_in):\n",
    "    str_in = str_in.replace(',' , '')\n",
    "    str_in = re.search(r'\\d+', str_in).group()\n",
    "    return str_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the extra columns that we're going to populate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_add = ['total faculty','inter faculty','total student','total inter']\n",
    "qs_df = pd.concat([qs_df, pd.DataFrame(columns=columns_to_add)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The page containing the additional data we're looking for is already contained in the `url` field of our `DataFrame`. Each value we're looking for is contained in a `<div>` tag with `class=` the data we're looking for, within this tag is another `<div>` tag with `class=\"number\"` which has the actual numeric value. We're therefore going to parse the page for each university and use `BeautifulSoup` to find all these tags. As there are several of them on each page, we'll double-check that they're  all the same.\n",
    "The following step is very slow, it has to parse a lot of html for 200 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for idx in qs_df.index:\n",
    "#for idx in range(30,200): # you can uncomment this to only parse the first few universities\n",
    "    page = requests.get(qs_url + qs_df.loc[idx]['url']) # GET the page for one university\n",
    "    soup = BeautifulSoup(page.text, 'html.parser') # parse it with bs4\n",
    "\n",
    "    for column in columns_to_add:\n",
    "        try:\n",
    "            wrapper = soup.find_all('div', class_=column) # find the tag of interest\n",
    "            if not wrapper:\n",
    "                print('No data for', qs_df.loc[idx]['title'], 'concerning', column)\n",
    "            values = np.zeros(len(wrapper))\n",
    "            for i in range(0,len(wrapper)): # if there are several tags, we'll check they have the same values\n",
    "                values[i] = xtract_number(wrapper[0].find('div', class_='number').string)\n",
    "                if i>0 and values[i] != values[i-1]:\n",
    "                   raise Exception('Numerical values for', qs_df.loc[idx]['title'], 'are different throughout the HTML') \n",
    "                else:\n",
    "                    qs_df.loc[idx][column] = values[0]\n",
    "            \n",
    "        except IndexError:\n",
    "            print('No data for', qs_df.loc[idx]['title'], 'concerning', column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can see that some data is missing for New York University and the Indian Institute of Science.\n",
    "Going to the website and checking this by hand does indeed show that these pieces of information are missing. We'll therefore leave these as NaN to signify the missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the time to GET and parse all this HTML, we've stored the `DataFrame` in a pickle for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump( qs_df, open( \"qs_dataframe.p\", \"wb\" ) )\n",
    "qs_df = pickle.load( open( \"qs_dataframe.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's drop some of the extra columns that we don't really need, they're still in the pickle if we need them later. We'll also change the columns to floats for our calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_df.drop(['core_id', 'guide', 'logo', 'nid', 'url', 'cc', 'score', 'stars'], axis=1, inplace=True)\n",
    "qs_df.rank_display = qs_df.rank_display.str.replace('=','')\n",
    "qs_df.rename(columns={'title':'name'}, inplace=True) # to merge on a column they need the same name in both dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_cols = ['rank_display', 'total faculty', 'inter faculty', 'total student', 'total inter']\n",
    "for col in int_cols:\n",
    "    qs_df.loc[:,col] = qs_df.loc[:,col].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which are the best universities?\n",
    "\n",
    "We are now going to compare the best universities in terms of ratio between faculty members: students and % of international students. Let's add these columns, they are merely operations involving the other columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_df['faculty:students ratio'] = qs_df['total faculty']/qs_df['total student']\n",
    "qs_df['% international students'] = 100*qs_df['total inter']/qs_df['total student']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_df.sort_values('faculty:students ratio' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_df.sort_values('% international students' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating by Country\n",
    "\n",
    "We'll first create a new `DataFrame` which will have info aggregated by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_country = pd.DataFrame(columns=['country'] + columns_to_add )\n",
    "qs_country['country'] = qs_df['country'].unique()\n",
    "qs_country.set_index('country', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the totals per country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for country in qs_df['country'].unique():\n",
    "    sums = qs_df[qs_df['country'] == country][columns_to_add].sum()\n",
    "    qs_country.loc[country][columns_to_add] = sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's calculate the same two stats that we did per university before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_country['faculty:students ratio'] = qs_country['total faculty']/qs_country['total student']\n",
    "qs_country['% international students'] = 100*qs_country['total inter']/qs_country['total student']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_country.sort_values('faculty:students ratio' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_country.sort_values('% international students' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating by region\n",
    "Let's do the same thing but grouping per region now, we'll do this in the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_region = pd.DataFrame(columns=['region'] + columns_to_add )\n",
    "qs_region['region'] = qs_df['region'].unique()\n",
    "qs_region.set_index('region', inplace=True)\n",
    "qs_region.head()\n",
    "\n",
    "for region in qs_df['region'].unique():\n",
    "    sums = qs_df[qs_df['region'] == region][columns_to_add].sum()\n",
    "    qs_region.loc[region][columns_to_add] = sums\n",
    "    \n",
    "qs_region['faculty:students ratio'] = qs_region['total faculty']/qs_region['total student']\n",
    "qs_region['% international students'] = 100*qs_region['total inter']/qs_region['total student']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_region.sort_values('faculty:students ratio' , ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qs_region.sort_values('% international students' , ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Scraping top 200 universities from Times Higher Education\n",
    "\n",
    "We have the same issue as before, the HTML from the given url doesn't contain the data that we actually\n",
    "want, rather it is loaded with a jQuery to a `json` somewhere else on the site. Using Postman and inspecting the html, there is only one `json` loaded on the ranking page, so we'll simply do some string handling to extract\n",
    "the url of interest from the HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_url = 'https://www.timeshighereducation.com/world-university-rankings/2018/world-ranking'\n",
    "resp = requests.get(times_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to extract the url we want, we're first going to find where the \"json\" at the end of the url is located. We'll then use `rfind` to find the \"http\" at the beginning of this url."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop = resp.text.find('json')\n",
    "start = resp.text.rfind('http', 0 , stop)\n",
    "times_data_url = resp.text[start:stop+len('json')]\n",
    "print(times_data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we need to filter out all the backlashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_data_url = times_data_url.replace('\\\\' , '')\n",
    "print(times_data_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_data = requests.get(times_data_url)\n",
    "times_parsed = times_data.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got some other keys than just the `data` one, but they don't seem of use for what we're looking for. `location` is already contained in the main `data` key-value pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_parsed.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, it looks like the `list` we get in the `data` key is conveniently organised from highest to lowest ranked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_parsed['data'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our `DataFrame` containing the top 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_df = pd.DataFrame()\n",
    "for i in range(0,200):\n",
    "    times_df = times_df.append(times_parsed['data'][i], ignore_index=True)\n",
    "print(times_df.shape)\n",
    "times_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got a lot more information this time, let's get rid of the columns we're not interested in, after backing it up to a pickle. We'll rename some of the columns too, to make it consistent with the previous `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle.dump( times_df, open( \"times_dataframe.p\", \"wb\" ) )\n",
    "times_df = pickle.load( open( \"times_dataframe.p\", \"rb\" ) )\n",
    "times_df = times_df[['location','name','rank','stats_student_staff_ratio','stats_number_students','stats_pc_intl_students']]\n",
    "times_df.rename(columns={'location':'country', 'stats_number_students':'total student','rank':'rank_display','stats_pc_intl_students':'% international students'}, inplace=True)\n",
    "times_df.rank_display = times_df.rank_display.str.replace('=','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now add region information based on what we have from the previous `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_df['region'] = np.nan\n",
    "for country in times_df['country'].unique():\n",
    "    try:\n",
    "        times_df.loc[times_df['country'] == country, 'region'] = qs_df[qs_df['country'] == country]['region'].iloc[0]\n",
    "    except IndexError:\n",
    "        print('No region info for', country)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're missing some region info about Luxembourg and the Russian Federation, so we'll add this by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_df.loc[times_df['country'] == 'Luxembourg', 'region'] = 'Europe'\n",
    "times_df.loc[times_df['country'] == 'Russian Federation', 'region'] = 'Europe' \n",
    "# in the previous data Russia is assigned to the Europe region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best universities\n",
    "\n",
    "We'll change types to floats where we need it. We also need to do a little bit of string cleaning before handing it over to `Pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_df.loc[:,'total student'] = times_df.loc[:,'total student'].str.replace(',' , '')\n",
    "times_df.loc[:,'% international students'] = times_df.loc[:,'% international students'].str.replace('%' , '')\n",
    "\n",
    "columns = ['rank_display', 'stats_student_staff_ratio', 'total student', '% international students']\n",
    "for col in columns:\n",
    "    times_df.loc[:,col] = times_df.loc[:,col].astype(float)\n",
    "times_df['faculty:students ratio'] = 1/times_df['stats_student_staff_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_df.sort_values('faculty:students ratio' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_df.sort_values('% international students' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've only got the % international students and students:staff ratio in the data from the Times, so we'll calculate the number of staff and international students from this data. Note that we don't have any info concerning % international faculty from the Times data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_df['total inter'] = times_df['total student']*times_df['% international students']/100\n",
    "times_df['total inter'] = times_df['total inter'].astype(int) # rounding it off to an integer\n",
    "times_df['total faculty'] = times_df['total student']*times_df['faculty:students ratio']\n",
    "times_df['total faculty'] = times_df['total faculty'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_add = ['total faculty', 'total student', 'total inter']\n",
    "times_country = pd.DataFrame(columns=['country'] + columns_to_add )\n",
    "times_country['country'] = times_df['country'].unique()\n",
    "times_country.set_index('country', inplace=True)\n",
    "\n",
    "for country in times_df['country'].unique():\n",
    "    sums = times_df[times_df['country'] == country][columns_to_add].sum()\n",
    "    times_country.loc[country][columns_to_add] = sums\n",
    "\n",
    "times_country['faculty:students ratio'] = times_country['total faculty']/times_country['total student']\n",
    "times_country['% international students'] = 100*times_country['total inter']/times_country['total student']\n",
    "times_country.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_country.sort_values('faculty:students ratio' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_country.sort_values('% international students' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping by region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_add = ['total faculty', 'total student', 'total inter']\n",
    "times_region = pd.DataFrame(columns=['region'] + columns_to_add )\n",
    "times_region['region'] = times_df['region'].unique()\n",
    "times_region.set_index('region', inplace=True)\n",
    "\n",
    "for region in times_df['region'].unique():\n",
    "    sums = times_df[times_df['region'] == region][columns_to_add].sum()\n",
    "    times_region.loc[region, columns_to_add] = sums\n",
    "    \n",
    "times_region['faculty:students ratio'] = times_region['total faculty']/times_region['total student']\n",
    "times_region['% international students'] = 100*times_region['total inter']/times_region['total student']\n",
    "times_region.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_region.sort_values('faculty:students ratio' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times_region.sort_values('% international students' , ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Merging both `DataFrames`\n",
    "\n",
    "As we've been looking at the number of students and faculty as well as whether they're international or not, we'll only keep this data in the merged `DataFrame`. It already looks like we only recover about half of the universities when we try to match them by name, 105 out of 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(set(times_df['name'].unique()).intersection(qs_df['name'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to see if we can increase this match by using the str.contains() method. If one of the `dfs` has a name that is extended from the other, we'll simplify this back to the shorter name. We can see that we recover more than 30 universities this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "diff1 = list(set(times_df['name'].unique()).difference(qs_df['name'].unique()))\n",
    "diff2 = list(set(qs_df['name'].unique()).difference(times_df['name'].unique()))\n",
    "\n",
    "for i in diff1:\n",
    "    if len(qs_df[qs_df['name'].str.contains(i)]) > 0:\n",
    "        qs_df.loc[qs_df['name'].str.contains(i), 'name'] = i\n",
    "for i in diff2:\n",
    "    if len(times_df[times_df['name'].str.contains(i)]) > 0:\n",
    "        times_df.loc[times_df['name'].str.contains(i),'name'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(set(times_df['name'].unique()).intersection(qs_df['name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mrg_df = times_df.merge(qs_df, how='inner', on='name')\n",
    "print(mrg_df.shape)\n",
    "mrg_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mrg_df.sort_index(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can simply perform a correlation between all columns to get an overal look at our data. We'll use Spearman's correlation coefficient, as we want to capture more than just linear relationships. Inspection (not shown here) with Pearson's showed similar trends.\n",
    "\n",
    "This actually gives us two pieces of information. Fist of all we can identify where our datasets diverge by comparing the same metric between both sets. Keep in mind this is correlation, so it will show a difference in trend of these numbers, not absolute value. Firstly we see that most data between sets have correlation of > 90%, however it seems that what consists of 'faculty' is rather different between the two. Indeed the topuniversities data mentions 'number of academic faculty staff' whereas the times only says 'staff'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mrg_df.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the stronger tends (Spearman's between roughly 0.4-0.6) is between the % of international students and total international students, suggesting that\n",
    "more international students there are in a university, the higher the fraction of international students. This could\n",
    "be due to a sort of \"sheep\" effect, where there are already large established international student communities is where they also represent a majority of the student population.\n",
    "\n",
    "There are a few weak trends we can observe, with either Spearman's coefficients roughly between 0.3 and 0.4\n",
    "\n",
    "* Spearman's suggests that there is some relationship between the % of international students and the number of international faculty members, the various correlation coefficients range between 0.29 and 0.41.\n",
    "* They also show some inverse relationship between the number of students and the & international students, suggesting that universities with more students have less of a fraction international students\n",
    "\n",
    "Again, these are rather weak trends, and would need further investigating to conclude something from them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Finding the 'best' university\n",
    "\n",
    "I believe that the ranking is the number one metric for trying to judge if a university is best. These two datasets have already synthesized lots of information into coming up with this ranking, so we might as well use it. We should try to take into account the difference in rankings in the two data sets. \n",
    "\n",
    "The quality of education students receive is also linked to how many staff are available to teach. Of course, a high faculty:student ratio does not automatically mean that all these extra faculty members are helping to teach students, but it's at least an opportunity for students to receive a better education.\n",
    "\n",
    "We'll therefore use a weighted sum of means to estimate the 'best' university based on both sets. We'll take the mean of the rank from both data sets, and add the mean of the student:staff ratio with a lower weight to re-rank the universities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mrg_df['mean_rank'] = mrg_df[['rank_display_x','rank_display_y']].mean(axis=1)\n",
    "mrg_df['mean_%_inter'] = mrg_df[['% international students_x','% international students_x']].mean(axis=1)/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mrg_df['new_rank'] = mrg_df['mean_rank'] - mrg_df['mean_%_inter']\n",
    "mrg_df.set_index('new_rank', inplace=True)\n",
    "mrg_df.sort_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
