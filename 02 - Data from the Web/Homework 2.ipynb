{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping info from topuniversities.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://www.topuniversities.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial postman/parsing\n",
    "Trying to get the url which contains the actual data that we want to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get(base_url + '/university-rankings/world-university-rankings/2018')\n",
    "soup = BeautifulSoup(resp.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts = soup.find_all('script', type='text/javascript')\n",
    "len(scripts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "for script in scripts:\n",
    "    if script.text.find('rank_url')!= -1:\n",
    "        print(i)\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts[28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts[28].text[58:]\n",
    "len(scripts[28].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts[28].text.find('rank_url')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts[28].text[14778:15178]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to parse headers to a python dict?\n",
    "resp.headers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual ranking data which is shown on the page is generated with a request to `rank_url`, therefore it is this\n",
    "linke that we'll need to GET to extract all the data we're interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the main data and putting everything into a `DataFrame`\n",
    "Scraping everything that is contained in the `rank_url`. This is the majority of what we are interested in, the faculty and student data are contained on another page that is specific to each university. This will be scraped afterwards in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_url = 'https://www.topuniversities.com/sites/default/files/qs-rankings-data/357051.txt'\n",
    "rank_data = requests.get(rank_url)\n",
    "parsed_data = rank_data.json()\n",
    "parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(parsed_data) # dict\n",
    "type(parsed_data['data']) #list\n",
    "# List is already organised based on rank (with indexing starting at 0):\n",
    "parsed_data['data'][3]['rank_display']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_data['data'][197]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now put all of this data into a single `DataFrame`. We're only interested in the top 200 universities, so we'll ignore the rest of the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_data = pd.DataFrame()\n",
    "for i in range(0,200):\n",
    "    qs_data = qs_data.append(parsed_data['data'][i], ignore_index=True)\n",
    "print(qs_data.shape)\n",
    "qs_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the specific page for each university\n",
    "\n",
    "We will first define a handy little function to extract numbers from strings with newlines and commas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xtract_number(str):\n",
    "    str = str.replace(',' , '')\n",
    "    str = re.search(r'\\d+', str).group()\n",
    "    return str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the extra columns that we're going to populate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_add = ['total faculty','inter faculty','total student','total inter']\n",
    "qs_data = pd.concat([qs_data, pd.DataFrame(columns=columns_to_add)], axis=1)\n",
    "qs_data.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional information is contained in the following tags\n",
    "<h3> Number of international students\n",
    "<h3> Number of students\n",
    "<h3> Number of academic faculty staff --> <div class=\"anno\">In total & <div class=\"anno\">International"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following step is very slow, it has to parse a lot of html for 200 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in qs_data.index:\n",
    "#for idx in [199]:\n",
    "    page = requests.get(base_url + qs_data.loc[idx]['url'])\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "    for column in columns_to_add:\n",
    "        try:\n",
    "            wrapper = soup.find_all('div',class_=column)\n",
    "            value = xtract_number(wrapper[1].find('div', class_='number').string)\n",
    "            qs_data.loc[idx][column] = value\n",
    "        except IndexError:\n",
    "            print('No data for', qs_data.loc[idx]['title'], 'concerning', column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we can see that some data is missing for New York University and the Indian Institute of Science.\n",
    "Going to the website and checking this by hand does indeed show that these pieces of information are missing. We'll therefore leave these as NaN to signify the missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_data.tail()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
